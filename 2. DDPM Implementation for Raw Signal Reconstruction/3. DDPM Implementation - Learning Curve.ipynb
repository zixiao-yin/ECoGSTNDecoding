{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab5204-3147-478c-9ce5-fa080c7aedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules at the top of the file\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "from dataset import ECoGDBSDataset\n",
    "from hydra import compose, initialize\n",
    "from utils import clear_gpu_memory, get_frequency_band_power\n",
    "from visualization import plot_and_save_psd_comparison\n",
    "from hyperparameter_tuning import tune_hyperparameters_safely\n",
    "from trainer import train_ecog_dbs_model\n",
    "from visualization import plot_and_save_psd_comparison, analyze_frequency_bands\n",
    "\n",
    "def get_all_predictions_fast(model, test_dataset, batch_size=2048, device=None):\n",
    "    \"\"\"Memory-optimized and faster prediction generation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Use model's device if none specified\n",
    "    if device is None:\n",
    "        device = model.device\n",
    "    \n",
    "    # Use larger batch size and enable cuda optimizations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "    # Create DataLoader with optimized settings\n",
    "    loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    # Pre-allocate lists with estimated size\n",
    "    n_samples = len(test_dataset)\n",
    "    real_dbs_all = []\n",
    "    imputed_dbs_all = []\n",
    "    \n",
    "    print(f\"\\nProcessing {n_samples} samples in {len(loader)} batches...\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():  # Use mixed precision\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                # Move data to device\n",
    "                ecog_batch = batch['cond'].to(device, non_blocking=True)\n",
    "                real_dbs_batch = batch['signal'].numpy()\n",
    "                \n",
    "                # Generate predictions\n",
    "                imputed_dbs_batch = model.sample(\n",
    "                    num_samples=ecog_batch.size(0),\n",
    "                    cond=ecog_batch,\n",
    "                    noise_type=\"alpha_beta\"\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                # Store results\n",
    "                real_dbs_all.append(real_dbs_batch)\n",
    "                imputed_dbs_all.append(imputed_dbs_batch)\n",
    "                \n",
    "                # Clear batch data\n",
    "                del ecog_batch\n",
    "                if batch_idx % 5 == 0:  # Reduce frequency of memory clearing\n",
    "                    clear_gpu_memory()\n",
    "                    print(f\"Processed {batch_idx + 1}/{len(loader)} batches\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction generation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        clear_gpu_memory()\n",
    "        raise\n",
    "    \n",
    "    # Concatenate results efficiently\n",
    "    print(\"\\nProcessing results...\")\n",
    "    real_dbs_all = np.concatenate(real_dbs_all, axis=0).squeeze()\n",
    "    imputed_dbs_all = np.concatenate(imputed_dbs_all, axis=0).squeeze()\n",
    "    \n",
    "    # Calculate band powers more efficiently\n",
    "    print(\"\\nCalculating frequency band powers...\")\n",
    "    bands = ['theta', 'alpha', 'beta', 'gamma']\n",
    "    band_powers_real = {band: [] for band in bands}\n",
    "    band_powers_imputed = {band: [] for band in bands}\n",
    "    \n",
    "    # Process band powers in larger chunks\n",
    "    chunk_size = 100\n",
    "    for i in range(0, len(real_dbs_all), chunk_size):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Computing band powers: {i}/{len(real_dbs_all)}\")\n",
    "        \n",
    "        # Process a chunk of signals at once\n",
    "        chunk_slice = slice(i, min(i + chunk_size, len(real_dbs_all)))\n",
    "        real_chunk = real_dbs_all[chunk_slice]\n",
    "        imputed_chunk = imputed_dbs_all[chunk_slice]\n",
    "        \n",
    "        # Process each signal in the chunk\n",
    "        for j in range(len(real_chunk)):\n",
    "            powers_real = get_frequency_band_power(real_chunk[j])\n",
    "            powers_imputed = get_frequency_band_power(imputed_chunk[j])\n",
    "            \n",
    "            for band in bands:\n",
    "                band_powers_real[band].append(powers_real[band])\n",
    "                band_powers_imputed[band].append(powers_imputed[band])\n",
    "    \n",
    "    # Convert to numpy arrays efficiently\n",
    "    for band in bands:\n",
    "        band_powers_real[band] = np.array(band_powers_real[band])\n",
    "        band_powers_imputed[band] = np.array(band_powers_imputed[band])\n",
    "    \n",
    "    # Calculate and include additional metrics\n",
    "    metrics = {\n",
    "        'real_dbs': real_dbs_all,\n",
    "        'imputed_dbs': imputed_dbs_all,\n",
    "        'band_powers_real': band_powers_real,\n",
    "        'band_powers_imputed': band_powers_imputed,\n",
    "        'test_set_size': n_samples,\n",
    "        'predictions_generated': len(real_dbs_all)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def create_dataset_splits(full_dataset, train_percentage, test_percentage=0.2, val_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits while maintaining a fixed test set and proportional validation set\n",
    "    \n",
    "    Args:\n",
    "        full_dataset: Complete dataset\n",
    "        train_percentage: Percentage of non-test data to use for training (0.1 to 0.9)\n",
    "        test_percentage: Percentage of data to reserve for testing\n",
    "        val_ratio: Ratio of validation set size relative to training set size\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    dataset_size = len(full_dataset)\n",
    "    test_size = int(test_percentage * dataset_size)\n",
    "    non_test_size = dataset_size - test_size\n",
    "    \n",
    "    # First create the train+val and test split\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    non_test_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [non_test_size, test_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # Calculate sizes for train and validation\n",
    "    # The validation set size will be proportional to the training set size\n",
    "    train_size = int(train_percentage * non_test_size)\n",
    "    val_size = int(train_size * val_ratio)  # Validation set is 20% the size of training set\n",
    "    \n",
    "    # Ensure we don't exceed the available non-test data\n",
    "    if train_size + val_size > non_test_size:\n",
    "        val_size = non_test_size - train_size\n",
    "    \n",
    "    # Split non_test_dataset into train and validation\n",
    "    train_dataset, val_dataset, _ = random_split(\n",
    "        non_test_dataset,\n",
    "        [train_size, val_size, non_test_size - train_size - val_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def plot_training_size_trends(results_summary, output_dir):\n",
    "    \"\"\"Plot trends in model performance across different training sizes\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    train_sizes = [r['train_percentage'] * 100 for r in results_summary]\n",
    "    \n",
    "    # Plot correlation trends for each frequency band\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bands = ['theta', 'alpha', 'beta', 'gamma']\n",
    "    for band in bands:\n",
    "        correlations = [r['correlations'][band] for r in results_summary]\n",
    "        plt.plot(train_sizes, correlations, marker='o', label=f'{band} band')\n",
    "    \n",
    "    plt.xlabel('Training Data Size (%)')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.title('Model Performance vs Training Data Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, 'training_size_trends.png'))\n",
    "    plt.close()\n",
    "\n",
    "def process_single_subject_with_varying_sizes(ecog_path, dbs_path, output_dir, config_path):\n",
    "    \"\"\"Process a single subject's data with different training set sizes\"\"\"\n",
    "    # Extract subject ID\n",
    "    subject_id = os.path.basename(ecog_path).split('_')[0]\n",
    "    side = os.path.basename(ecog_path).split('_')[1][:4]\n",
    "    full_subject_id = f\"{subject_id}_{side}\"\n",
    "    \n",
    "    try:\n",
    "        # Create subject-specific output directory\n",
    "        subject_output_dir = os.path.join(output_dir, full_subject_id)\n",
    "        os.makedirs(subject_output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nProcessing subject {full_subject_id}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load data\n",
    "        print(\"\\nLoading data...\")\n",
    "        ecog_data = np.load(ecog_path)\n",
    "        dbs_data = np.load(dbs_path)\n",
    "        \n",
    "        # Initialize config\n",
    "        with initialize(version_base=None, config_path=config_path):\n",
    "            base_config = compose(config_name=\"53_config_ou_200\")\n",
    "        \n",
    "        # Add signal_channel and cond_dim to network config\n",
    "        base_config.network.signal_channel = 1\n",
    "        base_config.network.cond_dim = 3\n",
    "        \n",
    "        # Create full dataset\n",
    "        full_dataset = ECoGDBSDataset(ecog_data, dbs_data)\n",
    "        \n",
    "        # Training percentages to evaluate (including full dataset)\n",
    "        train_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        results_summary = []\n",
    "        \n",
    "        for train_pct in train_percentages:\n",
    "            print(f\"\\nEvaluating with {train_pct*100}% training data\")\n",
    "            \n",
    "            # Create splits with consistent test set\n",
    "            train_dataset, val_dataset, test_dataset = create_dataset_splits(\n",
    "                full_dataset, \n",
    "                train_percentage=train_pct\n",
    "            )\n",
    "            \n",
    "            print(f\"Split sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "            \n",
    "            # Create size-specific output directory\n",
    "            size_output_dir = os.path.join(subject_output_dir, f\"train_{int(train_pct*100)}pct\")\n",
    "            os.makedirs(size_output_dir, exist_ok=True)\n",
    "            \n",
    "            # Run hyperparameter tuning with reduced trials for efficiency\n",
    "            print(\"\\nRunning hyperparameter tuning...\")\n",
    "            best_params, _, final_config = tune_hyperparameters_safely(\n",
    "                ecog_data=ecog_data,\n",
    "                dbs_data=dbs_data,\n",
    "                base_config=base_config,\n",
    "                output_dir=size_output_dir,\n",
    "                n_trials=20,  # Reduced number of trials\n",
    "                timeout=3600*2,  # Reduced timeout\n",
    "                tuning_epochs=30,  # Reduced epochs for tuning\n",
    "                final_epochs=200  # Reduced final epochs\n",
    "            )\n",
    "            \n",
    "            # Train final model with best parameters\n",
    "            print(\"\\nTraining final model...\")\n",
    "            diffusion_model, _, _ = train_ecog_dbs_model(\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                config=final_config\n",
    "            )\n",
    "            \n",
    "            # Generate predictions on test set\n",
    "            print(\"\\nGenerating predictions on test set...\")\n",
    "            results = get_all_predictions_fast(diffusion_model, test_dataset, batch_size=2048)\n",
    "            \n",
    "            # Plot and save PSD comparison\n",
    "            plot_and_save_psd_comparison(\n",
    "                results, \n",
    "                f\"{full_subject_id}_train{int(train_pct*100)}pct\", \n",
    "                size_output_dir\n",
    "            )\n",
    "            \n",
    "            # Analyze frequency bands\n",
    "            correlations = analyze_frequency_bands(results)\n",
    "            \n",
    "            # Save results for this training size\n",
    "            results_path = os.path.join(\n",
    "                size_output_dir, \n",
    "                f\"{subject_id}_{side}_train{int(train_pct*100)}pct_results.npz\"\n",
    "            )\n",
    "            np.savez(\n",
    "                results_path,\n",
    "                real_dbs=results['real_dbs'],\n",
    "                imputed_dbs=results['imputed_dbs'],\n",
    "                band_powers_real=results['band_powers_real'],\n",
    "                band_powers_imputed=results['band_powers_imputed'],\n",
    "                best_hyperparameters=best_params,\n",
    "                correlations=correlations,\n",
    "                train_size_percentage=train_pct\n",
    "            )\n",
    "            \n",
    "            # Store summary metrics\n",
    "            results_summary.append({\n",
    "                'train_percentage': train_pct,\n",
    "                'train_samples': len(train_dataset),\n",
    "                'correlations': correlations,\n",
    "                'best_params': best_params\n",
    "            })\n",
    "            \n",
    "            # Clear memory\n",
    "            del diffusion_model\n",
    "            clear_gpu_memory()\n",
    "        \n",
    "        # Save overall summary\n",
    "        summary_path = os.path.join(subject_output_dir, 'training_size_analysis_summary.npz')\n",
    "        np.savez(summary_path, results=results_summary)\n",
    "        \n",
    "        # Calculate and plot trends\n",
    "        plot_training_size_trends(results_summary, subject_output_dir)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\nCompleted processing {full_subject_id}\")\n",
    "        print(f\"Total duration: {duration:.2f} seconds\")\n",
    "        \n",
    "        return True, duration\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {full_subject_id}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        clear_gpu_memory()\n",
    "        return False, 0\n",
    "\n",
    "def batch_process_all_subjects_with_varying_sizes(input_dir, output_dir, config_path):\n",
    "    \"\"\"Process all subjects with varying training sizes\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all ECoG files\n",
    "    ecog_files = sorted(glob(os.path.join(input_dir, \"*ecog.npy\")))\n",
    "    total_subjects = len(ecog_files)\n",
    "    \n",
    "    print(f\"\\nFound {total_subjects} subjects to process\")\n",
    "    \n",
    "    # Initialize progress tracking\n",
    "    results_summary = []\n",
    "    total_start_time = time.time()\n",
    "    successful_processes = 0\n",
    "    \n",
    "    for subject_idx, ecog_file in enumerate(ecog_files, 1):\n",
    "        dbs_file = ecog_file.replace('ecog.npy', 'dbs.npy')\n",
    "        \n",
    "        if os.path.exists(dbs_file):\n",
    "            success, duration = process_single_subject_with_varying_sizes(\n",
    "                ecog_path=ecog_file,\n",
    "                dbs_path=dbs_file,\n",
    "                output_dir=output_dir,\n",
    "                config_path=config_path\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                successful_processes += 1\n",
    "            \n",
    "            results_summary.append({\n",
    "                'subject': os.path.basename(ecog_file),\n",
    "                'success': success,\n",
    "                'duration': duration\n",
    "            })\n",
    "    \n",
    "    # Save final summary\n",
    "    print(f\"\\nBatch processing completed!\")\n",
    "    print(f\"Successfully processed: {successful_processes}/{total_subjects}\")\n",
    "    \n",
    "    return results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951062a0-1c3c-4494-ac87-e944a155f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_dir = r'E:\\data_zixiao\\uscf_npy_3d_4s_nor_rmbad_9'\n",
    "output_dir = r'E:\\data_zixiao\\raw_prediction_56'\n",
    "config_path = \"../ecog_stn_icnworkstation/conf\"\n",
    "\n",
    "# Print system info\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Process all subjects with varying training sizes\n",
    "results = batch_process_all_subjects_with_varying_sizes(input_dir, output_dir, config_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
