{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf69b70-05d2-4212-87a4-4d88c1ded9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from hydra import compose, initialize\n",
    "from ntd.diffusion_model import Diffusion, Trainer\n",
    "from ntd.networks import AdaConv\n",
    "from ntd.utils.kernels_and_diffusion_utils import OUProcess\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import threading\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "def temporal_train_test_split(dataset, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Split dataset while preserving temporal order.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The full dataset\n",
    "        train_ratio: Proportion of data to use for training (default: 0.7)\n",
    "    \n",
    "    Returns:\n",
    "        train_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    # Calculate split index\n",
    "    total_length = len(dataset)\n",
    "    train_size = int(total_length * train_ratio)\n",
    "    \n",
    "    # Create train/test indices in order\n",
    "    train_indices = list(range(0, train_size))\n",
    "    test_indices = list(range(train_size, total_length))\n",
    "    \n",
    "    # Print split information\n",
    "    print(f\"\\nTemporal Split Information:\")\n",
    "    print(f\"Total samples: {total_length}\")\n",
    "    print(f\"Training samples: {len(train_indices)} (indices 0 to {train_size-1})\")\n",
    "    print(f\"Testing samples: {len(test_indices)} (indices {train_size} to {total_length-1})\")\n",
    "    \n",
    "    # Create train/test datasets using Subset\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "    \n",
    "class ECoGDBSDataset(Dataset):\n",
    "    def __init__(self, ecog_data, dbs_data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ecog_data: numpy array of shape (num_epochs, 3, sequence_length)\n",
    "            dbs_data: numpy array of shape (num_epochs, 1, sequence_length)\n",
    "        \"\"\"\n",
    "        # Convert inputs to numpy arrays if they're not already\n",
    "        if isinstance(ecog_data, torch.Tensor):\n",
    "            ecog_data = ecog_data.numpy()\n",
    "        if isinstance(dbs_data, torch.Tensor):\n",
    "            dbs_data = dbs_data.numpy()\n",
    "            \n",
    "        print(f\"Initial shapes:\")\n",
    "        print(f\"ECoG data: {ecog_data.shape}\")\n",
    "        print(f\"DBS data: {dbs_data.shape}\")\n",
    "        \n",
    "        # Ensure correct dimensions\n",
    "        if len(ecog_data.shape) != 3:\n",
    "            raise ValueError(f\"ECoG data should be 3D, got shape {ecog_data.shape}\")\n",
    "        if len(dbs_data.shape) != 3:\n",
    "            raise ValueError(f\"DBS data should be 3D, got shape {dbs_data.shape}\")\n",
    "        \n",
    "        # Ensure matching dimensions\n",
    "        if ecog_data.shape[0] != dbs_data.shape[0]:\n",
    "            raise ValueError(f\"Number of epochs don't match: {ecog_data.shape[0]} vs {dbs_data.shape[0]}\")\n",
    "        if ecog_data.shape[2] != dbs_data.shape[2]:\n",
    "            raise ValueError(f\"Sequence lengths don't match: {ecog_data.shape[2]} vs {dbs_data.shape[2]}\")\n",
    "        \n",
    "        # Store as numpy arrays\n",
    "        self.ecog_data = np.ascontiguousarray(ecog_data, dtype=np.float32)\n",
    "        self.dbs_data = np.ascontiguousarray(dbs_data, dtype=np.float32)\n",
    "        \n",
    "        print(f\"\\nProcessed shapes:\")\n",
    "        print(f\"ECoG data: {self.ecog_data.shape}\")\n",
    "        print(f\"DBS data: {self.dbs_data.shape}\")\n",
    "        \n",
    "        # Compute normalization statistics\n",
    "        self.ecog_mean = np.mean(self.ecog_data, axis=(0, 2), keepdims=True)\n",
    "        self.ecog_std = np.std(self.ecog_data, axis=(0, 2), keepdims=True)\n",
    "        self.dbs_mean = np.mean(self.dbs_data, axis=(0, 2), keepdims=True)\n",
    "        self.dbs_std = np.std(self.dbs_data, axis=(0, 2), keepdims=True)\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        self.ecog_std[self.ecog_std == 0] = 1\n",
    "        self.dbs_std[self.dbs_std == 0] = 1\n",
    "        \n",
    "        # Store number of epochs\n",
    "        self.num_epochs = ecog_data.shape[0]\n",
    "        \n",
    "        # Thread lock for safety\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_epochs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with self.lock:\n",
    "            try:\n",
    "                # Get data\n",
    "                ecog_sample = self.ecog_data[idx].copy()  # Should be (3, 1000)\n",
    "                dbs_sample = self.dbs_data[idx].copy()    # Should be (1, 1000)\n",
    "                \n",
    "                # Normalize\n",
    "                ecog_sample = (ecog_sample - self.ecog_mean[0]) / self.ecog_std[0]\n",
    "                dbs_sample = (dbs_sample - self.dbs_mean[0]) / self.dbs_std[0]\n",
    "                \n",
    "                # Convert to torch tensors\n",
    "                ecog_tensor = torch.from_numpy(ecog_sample).float()\n",
    "                dbs_tensor = torch.from_numpy(dbs_sample).float()\n",
    "                \n",
    "                # Final shape check\n",
    "                assert dbs_tensor.shape == (1, 1000), f\"Expected DBS shape (1, 1000), got {dbs_tensor.shape}\"\n",
    "                assert ecog_tensor.shape == (3, 1000), f\"Expected ECoG shape (3, 1000), got {ecog_tensor.shape}\"\n",
    "                \n",
    "                return {\n",
    "                    \"signal\": dbs_tensor,\n",
    "                    \"cond\": ecog_tensor\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing index {idx}: {str(e)}\")\n",
    "                raise\n",
    "def train_ecog_dbs_model(ecog_data, dbs_data, config_path):\n",
    "    \"\"\"Train DDPM model for ECoG to DBS imputation with temporal split.\"\"\"\n",
    "    \n",
    "    # Initialize config\n",
    "    with initialize(version_base=None, config_path=config_path):\n",
    "        cfg = compose(config_name=\"config_ou\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = ECoGDBSDataset(ecog_data, dbs_data)\n",
    "    \n",
    "    # Use temporal split instead of random split\n",
    "    train_dataset, test_dataset = temporal_train_test_split(\n",
    "        dataset, \n",
    "        train_ratio=cfg.dataset.train_test_split\n",
    "    )\n",
    "    \n",
    "    # Create data loader (without shuffling for test set)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.optimizer.train_batch_size,\n",
    "        shuffle=True,  # Can still shuffle training data\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize network\n",
    "    network = AdaConv(\n",
    "        signal_length=cfg.dataset.signal_length,\n",
    "        signal_channel=1,  # DBS data has 1 channel\n",
    "        cond_dim=3,       # ECoG data has 3 channels\n",
    "        hidden_channel=cfg.network.hidden_channel,\n",
    "        in_kernel_size=cfg.network.in_kernel_size,\n",
    "        out_kernel_size=cfg.network.out_kernel_size,\n",
    "        slconv_kernel_size=cfg.network.slconv_kernel_size,\n",
    "        num_scales=cfg.network.num_scales,\n",
    "        num_blocks=cfg.network.num_blocks,\n",
    "        num_off_diag=cfg.network.num_off_diag,\n",
    "        use_pos_emb=cfg.network.use_pos_emb,\n",
    "        padding_mode=cfg.network.padding_mode,\n",
    "        use_fft_conv=cfg.network.use_fft_conv,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize noise process (OU Process for 1/f noise)\n",
    "    ou_process = OUProcess(\n",
    "        cfg.diffusion_kernel.sigma_squared,\n",
    "        cfg.diffusion_kernel.ell,\n",
    "        cfg.dataset.signal_length\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize diffusion model\n",
    "    diffusion = Diffusion(\n",
    "        network=network,\n",
    "        noise_sampler=ou_process,\n",
    "        mal_dist_computer=ou_process,\n",
    "        diffusion_time_steps=cfg.diffusion.diffusion_steps,\n",
    "        schedule=cfg.diffusion.schedule,\n",
    "        start_beta=cfg.diffusion.start_beta,\n",
    "        end_beta=cfg.diffusion.end_beta,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        network.parameters(),\n",
    "        lr=cfg.optimizer.lr,\n",
    "        weight_decay=cfg.optimizer.weight_decay,\n",
    "    )\n",
    "    \n",
    "    # Custom Trainer class to handle our data format\n",
    "    class CustomTrainer:\n",
    "        def __init__(self, model, data_loader, optimizer, device):\n",
    "            self.model = model\n",
    "            self.data_loader = data_loader\n",
    "            self.optimizer = optimizer\n",
    "            self.device = device\n",
    "        \n",
    "        def train_epoch(self):\n",
    "            batchwise_losses = []\n",
    "            for batch in self.data_loader:\n",
    "                # Extract signal and condition from batch\n",
    "                sig_batch = batch[\"signal\"].to(self.device)\n",
    "                cond_batch = batch[\"cond\"].to(self.device)\n",
    "                \n",
    "                batch_size = sig_batch.shape[0]\n",
    "                \n",
    "                # Train step\n",
    "                batch_loss = self.model.train_batch(sig_batch, cond=cond_batch)\n",
    "                batch_loss = torch.mean(batch_loss)\n",
    "                \n",
    "                batchwise_losses.append((batch_size, batch_loss.item()))\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                # Optional: Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                \n",
    "            return batchwise_losses\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = CustomTrainer(diffusion, train_loader, optimizer, device)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(cfg.optimizer.num_epochs):\n",
    "        try:\n",
    "            print(f\"\\nEpoch {epoch}\")\n",
    "            batchwise_losses = trainer.train_epoch()\n",
    "            \n",
    "            # Calculate epoch loss\n",
    "            epoch_loss = 0\n",
    "            total_samples = 0\n",
    "            for batch_size, batch_loss in batchwise_losses:\n",
    "                epoch_loss += batch_size * batch_loss\n",
    "                total_samples += batch_size\n",
    "            epoch_loss /= total_samples\n",
    "            \n",
    "            print(f\"Average loss: {epoch_loss:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoch {epoch}:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "            \n",
    "    return diffusion, train_dataset, test_dataset\n",
    "\n",
    "def get_frequency_band_power(signal_data, fs=250, nperseg=256):\n",
    "    \"\"\"Calculate power in different frequency bands\"\"\"\n",
    "    f, psd = signal.welch(signal_data, fs=fs, nperseg=nperseg)\n",
    "    \n",
    "    # Define frequency bands\n",
    "    bands = {\n",
    "        'theta': (4, 8),\n",
    "        'alpha': (8, 13),\n",
    "        'beta': (13, 35),\n",
    "        'gamma': (35, 100)\n",
    "    }\n",
    "    \n",
    "    # Calculate power in each band\n",
    "    powers = {}\n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        # Find frequencies within band\n",
    "        mask = (f >= low_freq) & (f <= high_freq)\n",
    "        # Calculate total power in band\n",
    "        powers[band_name] = np.trapz(psd[mask], f[mask])\n",
    "    \n",
    "    return powers\n",
    "\n",
    "def get_all_predictions_fast(model, test_dataset, batch_size=64):\n",
    "    \"\"\"Memory-optimized prediction generation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create DataLoader\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    total_batches = len(loader)\n",
    "    \n",
    "    # Initialize lists\n",
    "    real_dbs_all = []\n",
    "    imputed_dbs_all = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(test_dataset)} samples in {total_batches} batches...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(loader, desc=\"Generating predictions\")):\n",
    "            # Move data to device\n",
    "            ecog_batch = batch['cond'].to(model.device)\n",
    "            real_dbs_batch = batch['signal'].cpu().numpy()\n",
    "            \n",
    "            # Generate predictions\n",
    "            imputed_dbs_batch = model.sample(\n",
    "                num_samples=ecog_batch.size(0),\n",
    "                cond=ecog_batch,\n",
    "                noise_type=\"alpha_beta\"\n",
    "            ).cpu().numpy()\n",
    "            \n",
    "            # Store results and immediately clear GPU tensors\n",
    "            real_dbs_all.append(real_dbs_batch)\n",
    "            imputed_dbs_all.append(imputed_dbs_batch)\n",
    "            \n",
    "            # Clear batch data\n",
    "            del ecog_batch\n",
    "            if batch_idx % 10 == 0:  # Clear cache periodically\n",
    "                clear_gpu_memory()\n",
    "    \n",
    "    # Concatenate results\n",
    "    print(\"\\nProcessing results...\")\n",
    "    real_dbs_all = np.concatenate(real_dbs_all, axis=0).squeeze()\n",
    "    imputed_dbs_all = np.concatenate(imputed_dbs_all, axis=0).squeeze()\n",
    "    \n",
    "    # Calculate band powers\n",
    "    print(\"\\nCalculating frequency band powers...\")\n",
    "    band_powers_real = {}\n",
    "    band_powers_imputed = {}\n",
    "    \n",
    "    for band in ['theta', 'alpha', 'beta', 'gamma']:\n",
    "        band_powers_real[band] = []\n",
    "        band_powers_imputed[band] = []\n",
    "    \n",
    "    for i in tqdm(range(len(real_dbs_all)), desc=\"Computing band powers\"):\n",
    "        powers_real = get_frequency_band_power(real_dbs_all[i])\n",
    "        powers_imputed = get_frequency_band_power(imputed_dbs_all[i])\n",
    "        \n",
    "        for band in powers_real:\n",
    "            band_powers_real[band].append(powers_real[band])\n",
    "            band_powers_imputed[band].append(powers_imputed[band])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    for band in band_powers_real:\n",
    "        band_powers_real[band] = np.array(band_powers_real[band])\n",
    "        band_powers_imputed[band] = np.array(band_powers_imputed[band])\n",
    "    \n",
    "    return {\n",
    "        'real_dbs': real_dbs_all,\n",
    "        'imputed_dbs': imputed_dbs_all,\n",
    "        'band_powers_real': band_powers_real,\n",
    "        'band_powers_imputed': band_powers_imputed\n",
    "    }\n",
    "\n",
    "def analyze_frequency_bands(results):\n",
    "    \"\"\"Analyze frequency band powers and their correlations\"\"\"\n",
    "    band_powers_real = results['band_powers_real']\n",
    "    band_powers_imputed = results['band_powers_imputed']\n",
    "    \n",
    "    # Calculate correlations for each band\n",
    "    correlations = {}\n",
    "    for band in band_powers_real:\n",
    "        corr = pearsonr(band_powers_real[band], band_powers_imputed[band])[0]\n",
    "        correlations[band] = corr\n",
    "    \n",
    "    # Print correlations\n",
    "    print(\"\\nFrequency Band Power Correlations:\")\n",
    "    for band, corr in correlations.items():\n",
    "        print(f\"{band.capitalize()}: {corr:.3f}\")\n",
    "    \n",
    "    # Create scatter plots for each band\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, band in enumerate(band_powers_real):\n",
    "        ax = axes[idx]\n",
    "        ax.scatter(band_powers_real[band], band_powers_imputed[band], \n",
    "                  alpha=0.5, label=f'r = {correlations[band]:.3f}')\n",
    "        \n",
    "        # Add diagonal line\n",
    "        min_val = min(band_powers_real[band].min(), band_powers_imputed[band].min())\n",
    "        max_val = max(band_powers_real[band].max(), band_powers_imputed[band].max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Real Power')\n",
    "        ax.set_ylabel('Imputed Power')\n",
    "        ax.set_title(f'{band.capitalize()} Band Power')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c87f1-f1d9-41cb-b41a-d58769e01da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Enhanced GPU memory clearing\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Clear CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        # Ensure CUDA synchronization\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Convert seconds to human-readable format\"\"\"\n",
    "    return str(timedelta(seconds=int(seconds)))\n",
    "\n",
    "def plot_and_save_psd_comparison(results, subject_id, output_dir):\n",
    "    \"\"\"Plot and save PSD comparison for a subject\"\"\"\n",
    "    try:\n",
    "        # Create figure directory if it doesn't exist\n",
    "        fig_dir = os.path.join(output_dir, 'psd_figures')\n",
    "        os.makedirs(fig_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"\\nCalculating average PSDs...\")\n",
    "        real_dbs = results['real_dbs']\n",
    "        imputed_dbs = results['imputed_dbs']\n",
    "        \n",
    "        # Calculate PSDs for all trials\n",
    "        f, psd_real_all = signal.welch(real_dbs[0], fs=250, nperseg=256)\n",
    "        psd_imputed_all = signal.welch(imputed_dbs[0], fs=250, nperseg=256)[1]\n",
    "        \n",
    "        for i in tqdm(range(1, len(real_dbs)), desc=\"Computing PSDs\"):\n",
    "            psd_real = signal.welch(real_dbs[i], fs=250, nperseg=256)[1]\n",
    "            psd_imputed = signal.welch(imputed_dbs[i], fs=250, nperseg=256)[1]\n",
    "            psd_real_all = np.vstack((psd_real_all, psd_real))\n",
    "            psd_imputed_all = np.vstack((psd_imputed_all, psd_imputed))\n",
    "        \n",
    "        # Calculate mean and std\n",
    "        psd_real_mean = np.mean(psd_real_all, axis=0)\n",
    "        psd_real_std = np.std(psd_real_all, axis=0)\n",
    "        psd_imputed_mean = np.mean(psd_imputed_all, axis=0)\n",
    "        psd_imputed_std = np.std(psd_imputed_all, axis=0)\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot real DBS\n",
    "        plt.semilogy(f, psd_real_mean, 'b', label='Real DBS (mean)', alpha=0.7)\n",
    "        plt.fill_between(f, \n",
    "                        psd_real_mean - psd_real_std, \n",
    "                        psd_real_mean + psd_real_std, \n",
    "                        color='b', alpha=0.2)\n",
    "        \n",
    "        # Plot imputed DBS\n",
    "        plt.semilogy(f, psd_imputed_mean, 'r', label='Imputed DBS (mean)', alpha=0.7)\n",
    "        plt.fill_between(f, \n",
    "                        psd_imputed_mean - psd_imputed_std, \n",
    "                        psd_imputed_mean + psd_imputed_std, \n",
    "                        color='r', alpha=0.2)\n",
    "        \n",
    "        # Add frequency band annotations\n",
    "        bands = {\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30),\n",
    "            'gamma': (30, 100)\n",
    "        }\n",
    "        \n",
    "        # Add frequency band shading and labels\n",
    "        colors = ['lightgray', 'lightblue', 'lightgreen', 'lightpink']\n",
    "        for (band, (fmin, fmax)), color in zip(bands.items(), colors):\n",
    "            plt.axvspan(fmin, fmax, color=color, alpha=0.2)\n",
    "            plt.text((fmin + fmax)/2, plt.ylim()[0]*1.1, band, \n",
    "                    horizontalalignment='center', verticalalignment='bottom')\n",
    "        \n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Power Spectral Density')\n",
    "        plt.title(f'Average Power Spectral Density with Std Dev\\n{subject_id}')\n",
    "        plt.xlim(0, 125)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save figure\n",
    "        fig_path = os.path.join(fig_dir, f'{subject_id}_psd_comparison.png')\n",
    "        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save PSD data\n",
    "        psd_data_path = os.path.join(fig_dir, f'{subject_id}_psd_data.npz')\n",
    "        np.savez(psd_data_path,\n",
    "                 frequencies=f,\n",
    "                 psd_real_mean=psd_real_mean,\n",
    "                 psd_real_std=psd_real_std,\n",
    "                 psd_imputed_mean=psd_imputed_mean,\n",
    "                 psd_imputed_std=psd_imputed_std)\n",
    "        \n",
    "        print(f\"PSD comparison saved to {fig_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting PSD comparison: {str(e)}\")\n",
    "        \n",
    "def process_single_subject(ecog_path, dbs_path, output_dir, config_path):\n",
    "    \"\"\"Process a single subject's data with PSD plotting\"\"\"\n",
    "    try:\n",
    "        # Extract subject ID from filename\n",
    "        subject_id = os.path.basename(ecog_path).split('_')[0]\n",
    "        side = os.path.basename(ecog_path).split('_')[1][:4]\n",
    "        full_subject_id = f\"{subject_id}_{side}\"\n",
    "        print(f\"\\nProcessing subject {full_subject_id}\")\n",
    "        \n",
    "        # Load data\n",
    "        with tqdm(total=2, desc=\"Loading data\") as pbar:\n",
    "            ecog_data = np.load(ecog_path)\n",
    "            pbar.update(1)\n",
    "            dbs_data = np.load(dbs_path)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        print(f\"Data shapes - ECoG: {ecog_data.shape}, DBS: {dbs_data.shape}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU memory before training: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "        \n",
    "        # Create output path\n",
    "        output_path = os.path.join(output_dir, f\"{subject_id}_{side}_prediction_results.npz\")\n",
    "        \n",
    "        # Start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\nTraining model...\")\n",
    "        diffusion_model, train_dataset, test_dataset = train_ecog_dbs_model(\n",
    "            ecog_data=ecog_data,\n",
    "            dbs_data=dbs_data,\n",
    "            config_path=config_path\n",
    "        )\n",
    "        \n",
    "        # Clear unnecessary training data\n",
    "        del ecog_data, dbs_data\n",
    "        clear_gpu_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU memory after training: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        print(\"\\nGenerating predictions...\")\n",
    "        results = get_all_predictions_fast(diffusion_model, test_dataset, \n",
    "                                           batch_size=2048)\n",
    "        \n",
    "        # Plot and save PSD comparison\n",
    "        plot_and_save_psd_comparison(results, full_subject_id, output_dir)\n",
    "        \n",
    "        # Clear model and datasets\n",
    "        del diffusion_model, train_dataset\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU memory after predictions: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "        \n",
    "        # Analyze and save results\n",
    "        print(\"\\nAnalyzing frequency bands...\")\n",
    "        correlations = analyze_frequency_bands(results)\n",
    "        \n",
    "        # Save results with progress bar\n",
    "        print(f\"\\nSaving results to {output_path}\")\n",
    "        with tqdm(total=1, desc=\"Saving results\") as pbar:\n",
    "            np.savez(output_path,\n",
    "                     real_dbs=results['real_dbs'],\n",
    "                     imputed_dbs=results['imputed_dbs'],\n",
    "                     band_powers_real=results['band_powers_real'],\n",
    "                     band_powers_imputed=results['band_powers_imputed'])\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Calculate duration\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\nCompleted in {format_time(duration)}\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        del results, test_dataset, correlations\n",
    "        clear_gpu_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Final GPU memory: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "        \n",
    "        return True, duration\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {full_subject_id}: {str(e)}\")\n",
    "        clear_gpu_memory()\n",
    "        return False, 0\n",
    "\n",
    "def batch_process_all_subjects(input_dir, output_dir, config_path):\n",
    "    \"\"\"Process all subjects with PSD plotting\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all ECoG files\n",
    "    ecog_files = sorted(glob(os.path.join(input_dir, \"*ecog.npy\")))\n",
    "    total_subjects = len(ecog_files)\n",
    "    \n",
    "    print(f\"\\nFound {total_subjects} subjects to process\")\n",
    "    time.sleep(1)  # Give time to read the message\n",
    "    \n",
    "    # Initialize progress tracking\n",
    "    results_summary = []\n",
    "    total_start_time = time.time()\n",
    "    successful_processes = 0\n",
    "    \n",
    "    # Create main progress bar\n",
    "    with tqdm(total=total_subjects, desc=\"Overall Progress\") as pbar:\n",
    "        for subject_idx, ecog_file in enumerate(ecog_files, 1):\n",
    "            # Get corresponding DBS file\n",
    "            dbs_file = ecog_file.replace('ecog.npy', 'dbs.npy')\n",
    "            \n",
    "            if os.path.exists(dbs_file):\n",
    "                # Display progress information\n",
    "                elapsed_time = time.time() - total_start_time\n",
    "                if successful_processes > 0:\n",
    "                    avg_time_per_subject = elapsed_time / successful_processes\n",
    "                    estimated_remaining = avg_time_per_subject * (total_subjects - subject_idx + 1)\n",
    "                    print(f\"\\nEstimated time remaining: {format_time(estimated_remaining)}\")\n",
    "                \n",
    "                print(f\"\\nProcessing subject {subject_idx}/{total_subjects}\")\n",
    "                print(f\"File: {os.path.basename(ecog_file)}\")\n",
    "                \n",
    "                # Process subject\n",
    "                start_time = time.time()\n",
    "                success, duration = process_single_subject(\n",
    "                    ecog_path=ecog_file,\n",
    "                    dbs_path=dbs_file,\n",
    "                    output_dir=output_dir,\n",
    "                    config_path=config_path\n",
    "                )\n",
    "                \n",
    "                if success:\n",
    "                    successful_processes += 1\n",
    "                \n",
    "                # Store results\n",
    "                results_summary.append({\n",
    "                    'subject': os.path.basename(ecog_file),\n",
    "                    'success': success,\n",
    "                    'duration': duration,\n",
    "                    'processed_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Save progress summary\n",
    "                with open(os.path.join(output_dir, 'processing_summary.txt'), 'w') as f:\n",
    "                    f.write(\"Processing Summary:\\n\")\n",
    "                    f.write(f\"Total subjects: {total_subjects}\\n\")\n",
    "                    f.write(f\"Completed: {subject_idx}\\n\")\n",
    "                    f.write(f\"Successful: {successful_processes}\\n\")\n",
    "                    f.write(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "                    \n",
    "                    for result in results_summary:\n",
    "                        f.write(f\"Subject: {result['subject']}\\n\")\n",
    "                        f.write(f\"Success: {result['success']}\\n\")\n",
    "                        f.write(f\"Duration: {format_time(result['duration'])}\\n\")\n",
    "                        f.write(f\"Processed at: {result['processed_at']}\\n\\n\")\n",
    "            else:\n",
    "                print(f\"Warning: No matching DBS file found for {ecog_file}\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Final summary\n",
    "    total_duration = time.time() - total_start_time\n",
    "    print(f\"\\nBatch processing completed!\")\n",
    "    print(f\"Total time: {format_time(total_duration)}\")\n",
    "    print(f\"Successfully processed: {successful_processes}/{total_subjects}\")\n",
    "    \n",
    "    # Create PSD figure directory\n",
    "    psd_dir = os.path.join(output_dir, 'psd_figures')\n",
    "    os.makedirs(psd_dir, exist_ok=True)\n",
    "    \n",
    "# Create a summary plot at the end\n",
    "def create_summary_plot():\n",
    "    try:\n",
    "        print(\"\\nCreating summary PSD plot...\")\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Get all PSD data files\n",
    "        psd_files = glob(os.path.join(psd_dir, '*_psd_data.npz'))\n",
    "        \n",
    "        # Plot each subject's PSD with lighter colors\n",
    "        for psd_file in psd_files:\n",
    "            data = np.load(psd_file)\n",
    "            subject = os.path.basename(psd_file).split('_psd_data.npz')[0]\n",
    "            \n",
    "            plt.semilogy(data['frequencies'], data['psd_real_mean'], \n",
    "                       alpha=0.2, color='blue')\n",
    "            plt.semilogy(data['frequencies'], data['psd_imputed_mean'], \n",
    "                       alpha=0.2, color='red')\n",
    "        \n",
    "        # Add frequency band annotations\n",
    "        bands = {\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30),\n",
    "            'gamma': (30, 100)\n",
    "        }\n",
    "        \n",
    "        # Add frequency band shading\n",
    "        colors = ['lightgray', 'lightblue', 'lightgreen', 'lightpink']\n",
    "        for (band, (fmin, fmax)), color in zip(bands.items(), colors):\n",
    "            plt.axvspan(fmin, fmax, color=color, alpha=0.2)\n",
    "            plt.text((fmin + fmax)/2, plt.ylim()[0]*1.1, band, \n",
    "                    horizontalalignment='center', verticalalignment='bottom')\n",
    "        \n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Power Spectral Density')\n",
    "        plt.title('Summary of All Subjects\\nBlue: Real DBS, Red: Imputed DBS')\n",
    "        plt.xlim(0, 125)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save summary plot\n",
    "        summary_path = os.path.join(psd_dir, 'summary_psd_comparison.png')\n",
    "        plt.savefig(summary_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Summary PSD plot saved to {summary_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating summary plot: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d80470-2cd0-4ebb-9039-86a424efb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def get_all_predictions_fast(model, test_dataset, device='cuda', batch_size=64):\n",
    "    \"\"\"Get all real and imputed DBS data from test dataset - faster version with progress bars\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create DataLoader\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    total_batches = len(loader)\n",
    "    \n",
    "    # Initialize lists\n",
    "    real_dbs_all = []\n",
    "    imputed_dbs_all = []\n",
    "    \n",
    "    # Main progress bar\n",
    "    print(f\"\\nProcessing {len(test_dataset)} samples in {total_batches} batches...\")\n",
    "    pbar = tqdm(total=total_batches, desc=\"Generating predictions\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Move data to device\n",
    "            ecog_batch = batch['cond'].to(device)\n",
    "            real_dbs_batch = batch['signal'].cpu().numpy()\n",
    "            \n",
    "            # Generate predictions in batch\n",
    "            imputed_dbs_batch = model.sample(\n",
    "                num_samples=ecog_batch.size(0),\n",
    "                cond=ecog_batch,\n",
    "                noise_type=\"alpha_beta\"\n",
    "            ).cpu().numpy()\n",
    "            \n",
    "            # Store results\n",
    "            real_dbs_all.append(real_dbs_batch)\n",
    "            imputed_dbs_all.append(imputed_dbs_batch)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    print(\"\\nProcessing results...\")\n",
    "    real_dbs_all = np.concatenate(real_dbs_all, axis=0).squeeze()\n",
    "    imputed_dbs_all = np.concatenate(imputed_dbs_all, axis=0).squeeze()\n",
    "    \n",
    "    # Calculate metrics (vectorized)\n",
    "    print(\"Calculating metrics...\")\n",
    "    mse_scores = np.mean((real_dbs_all - imputed_dbs_all) ** 2, axis=1)\n",
    "    \n",
    "    # Calculate correlations with progress bar\n",
    "    print(\"Calculating correlations...\")\n",
    "    corr_scores = []\n",
    "    for i in tqdm(range(len(real_dbs_all)), desc=\"Computing correlations\"):\n",
    "        corr = pearsonr(real_dbs_all[i], imputed_dbs_all[i])[0]\n",
    "        corr_scores.append(corr)\n",
    "    corr_scores = np.array(corr_scores)\n",
    "    \n",
    "    return {\n",
    "        'real_dbs': real_dbs_all,\n",
    "        'imputed_dbs': imputed_dbs_all,\n",
    "        'mse_scores': mse_scores,\n",
    "        'corr_scores': corr_scores\n",
    "    }\n",
    "\n",
    "def analyze_predictions_fast(results, max_samples_psd=1000):\n",
    "    \"\"\"Analyze predictions faster by using subset for PSD\"\"\"\n",
    "    real_dbs = results['real_dbs']\n",
    "    imputed_dbs = results['imputed_dbs']\n",
    "    mse_scores = results['mse_scores']\n",
    "    corr_scores = results['corr_scores']\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(f\"Number of test samples: {len(real_dbs)}\")\n",
    "    print(f\"\\nMSE scores:\")\n",
    "    print(f\"Mean: {np.mean(mse_scores):.6f}\")\n",
    "    print(f\"Std: {np.std(mse_scores):.6f}\")\n",
    "    print(f\"Min: {np.min(mse_scores):.6f}\")\n",
    "    print(f\"Max: {np.max(mse_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\nCorrelation scores:\")\n",
    "    print(f\"Mean: {np.mean(corr_scores):.6f}\")\n",
    "    print(f\"Std: {np.std(corr_scores):.6f}\")\n",
    "    print(f\"Min: {np.min(corr_scores):.6f}\")\n",
    "    print(f\"Max: {np.max(corr_scores):.6f}\")\n",
    "    \n",
    "    # Calculate PSD on a subset of samples\n",
    "    print(f\"\\nCalculating PSDs on {max_samples_psd} samples...\")\n",
    "    indices = np.random.choice(len(real_dbs), min(max_samples_psd, len(real_dbs)), replace=False)\n",
    "    \n",
    "    # Calculate PSDs with progress bar\n",
    "    psd_real_all = []\n",
    "    psd_imputed_all = []\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"Computing PSDs\"):\n",
    "        f, psd_real = signal.welch(real_dbs[idx], fs=250, nperseg=256)\n",
    "        _, psd_imputed = signal.welch(imputed_dbs[idx], fs=250, nperseg=256)\n",
    "        psd_real_all.append(psd_real)\n",
    "        psd_imputed_all.append(psd_imputed)\n",
    "    \n",
    "    psd_real_all = np.array(psd_real_all)\n",
    "    psd_imputed_all = np.array(psd_imputed_all)\n",
    "    \n",
    "    # Calculate mean and std of PSDs\n",
    "    psd_real_mean = np.mean(psd_real_all, axis=0)\n",
    "    psd_real_std = np.std(psd_real_all, axis=0)\n",
    "    psd_imputed_mean = np.mean(psd_imputed_all, axis=0)\n",
    "    psd_imputed_std = np.std(psd_imputed_all, axis=0)\n",
    "    \n",
    "    print(\"\\nGenerating plots...\")\n",
    "    \n",
    "    # Plot average PSD with std\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.semilogy(f, psd_real_mean, 'b', label='Real DBS (mean)', alpha=0.7)\n",
    "    plt.fill_between(f, \n",
    "                     psd_real_mean - psd_real_std, \n",
    "                     psd_real_mean + psd_real_std, \n",
    "                     color='b', alpha=0.2)\n",
    "    plt.semilogy(f, psd_imputed_mean, 'r', label='Imputed DBS (mean)', alpha=0.7)\n",
    "    plt.fill_between(f, \n",
    "                     psd_imputed_mean - psd_imputed_std, \n",
    "                     psd_imputed_mean + psd_imputed_std, \n",
    "                     color='r', alpha=0.2)\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power Spectral Density')\n",
    "    plt.title('Average Power Spectral Density with Std Dev')\n",
    "    plt.xlim(0, 125)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot histogram of correlation scores\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(corr_scores, bins=50, alpha=0.7)\n",
    "    plt.axvline(np.mean(corr_scores), color='r', linestyle='dashed', \n",
    "                label=f'Mean ({np.mean(corr_scores):.3f})')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Correlation Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'f': f,\n",
    "        'psd_real_mean': psd_real_mean,\n",
    "        'psd_real_std': psd_real_std,\n",
    "        'psd_imputed_mean': psd_imputed_mean,\n",
    "        'psd_imputed_std': psd_imputed_std\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Output path\n",
    "    output_path = r'D:\\data_zixiao\\raw_prediction_46\\dbs_prediction_results.npz'\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get predictions\n",
    "    print(\"Starting prediction generation...\")\n",
    "    results = get_all_predictions_fast(diffusion_model, test_dataset, batch_size=2048)\n",
    "    \n",
    "    # Analyze predictions\n",
    "    print(\"\\nStarting analysis...\")\n",
    "    psd_results = analyze_predictions_fast(results, max_samples_psd=1000)\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\nSaving results to {output_path}\")\n",
    "    np.savez(output_path,\n",
    "             real_dbs=results['real_dbs'],\n",
    "             imputed_dbs=results['imputed_dbs'],\n",
    "             mse_scores=results['mse_scores'],\n",
    "             corr_scores=results['corr_scores'],\n",
    "             frequencies=psd_results['f'],\n",
    "             psd_real_mean=psd_results['psd_real_mean'],\n",
    "             psd_real_std=psd_results['psd_real_std'],\n",
    "             psd_imputed_mean=psd_results['psd_imputed_mean'],\n",
    "             psd_imputed_std=psd_results['psd_imputed_std'])\n",
    "    \n",
    "    # End time\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nCompleted in {duration//60:.0f}m {duration%60:.1f}s\")\n",
    "    print(f\"Results saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
