{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace2294-68d8-44d3-99ae-16ad41557e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\1代码 通过ecog数据预测stn的beta\\data_upload\\2. DDPM Implementation for Raw Signal Reconstruction\\trainer.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4090\n",
      "Initial GPU memory allocated: 0.00 GB\n",
      "\n",
      "Found 1 subjects to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|                                                                          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject 1/1\n",
      "File: ddpm_demo_ecog.npy\n",
      "\n",
      "Processing subject ddpm_demo\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Running hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 09:22:40,461 - \n",
      "Initializing Memory-Safe Tuner:\n",
      "2025-10-29 09:22:40,463 - Available GPU Memory: 0.00 GB\n",
      "2025-10-29 09:22:40,464 - Using parameter bounds: {'hidden_channel': (48, 92), 'num_blocks': (3, 4), 'batch_size': (384, 768), 'diffusion_steps': (500, 1000)}\n",
      "2025-10-29 09:22:40,465 - Tuning epochs: 50\n",
      "2025-10-29 09:22:40,466 - Final epochs: 300\n",
      "[I 2025-10-29 09:22:40,467] A new study created in memory with name: no-name-2cea3322-df7b-4ef1-9253-8200693508ed\n",
      "2025-10-29 09:22:40,470 - \n",
      "Memory Analysis:\n",
      "2025-10-29 09:22:40,471 - Estimated memory requirement: 4.72 GB\n",
      "2025-10-29 09:22:40,473 - Current GPU memory usage: 0.00 GB\n",
      "2025-10-29 09:22:40,474 - \n",
      "================================================================================\n",
      "2025-10-29 09:22:40,479 - Starting Trial 0\n",
      "2025-10-29 09:22:40,480 - Parameters:\n",
      "2025-10-29 09:22:40,481 -   hidden_channel: 62\n",
      "2025-10-29 09:22:40,481 -   num_blocks: 4\n",
      "2025-10-29 09:22:40,482 -   diffusion_steps: 895\n",
      "2025-10-29 09:22:40,483 -   batch_size: 742\n",
      "2025-10-29 09:22:40,484 -   start_beta: 0.0003529637220466875\n",
      "2025-10-29 09:22:40,485 -   end_beta: 0.012562682440227632\n",
      "2025-10-29 09:22:40,486 -   lr: 1.920476593986205e-05\n",
      "2025-10-29 09:22:40,487 - ================================================================================\n",
      "\n",
      "C:\\Users\\admin\\1代码 通过ecog数据预测stn的beta\\data_upload\\2. DDPM Implementation for Raw Signal Reconstruction\\trainer.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\admin\\1代码 通过ecog数据预测stn的beta\\data_upload\\2. DDPM Implementation for Raw Signal Reconstruction\\trainer.py:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "D:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 summary:\n",
      "Average loss: 1146.438000\n",
      "Learning rate: 0.000001\n",
      "\n",
      "Epoch 1 summary:\n",
      "Average loss: 1144.544000\n",
      "Learning rate: 0.000002\n",
      "\n",
      "Epoch 2 summary:\n",
      "Average loss: 1143.922000\n",
      "Learning rate: 0.000003\n",
      "\n",
      "Epoch 3 summary:\n",
      "Average loss: 1141.240000\n",
      "Learning rate: 0.000004\n",
      "\n",
      "Epoch 4 summary:\n",
      "Average loss: 1134.876000\n",
      "Learning rate: 0.000005\n",
      "\n",
      "Epoch 5 summary:\n",
      "Average loss: 1126.530000\n",
      "Learning rate: 0.000007\n",
      "\n",
      "Epoch 6 summary:\n",
      "Average loss: 1119.042000\n",
      "Learning rate: 0.000009\n",
      "\n",
      "Epoch 7 summary:\n",
      "Average loss: 1105.176000\n",
      "Learning rate: 0.000011\n",
      "\n",
      "Epoch 8 summary:\n",
      "Average loss: 1090.314000\n",
      "Learning rate: 0.000013\n",
      "\n",
      "Epoch 9 summary:\n",
      "Average loss: 1071.826000\n",
      "Learning rate: 0.000015\n",
      "\n",
      "Epoch 10 summary:\n",
      "Average loss: 1050.254000\n",
      "Learning rate: 0.000016\n",
      "\n",
      "Epoch 11 summary:\n",
      "Average loss: 1026.411000\n",
      "Learning rate: 0.000018\n",
      "\n",
      "Epoch 12 summary:\n",
      "Average loss: 1000.393000\n",
      "Learning rate: 0.000018\n",
      "\n",
      "Epoch 13 summary:\n",
      "Average loss: 971.331000\n",
      "Learning rate: 0.000019\n",
      "\n",
      "Epoch 14 summary:\n",
      "Average loss: 940.234000\n",
      "Learning rate: 0.000019\n",
      "\n",
      "Epoch 15 summary:\n",
      "Average loss: 909.497000\n",
      "Learning rate: 0.000019\n",
      "\n",
      "Epoch 16 summary:\n",
      "Average loss: 875.370000\n",
      "Learning rate: 0.000019\n",
      "\n",
      "Epoch 17 summary:\n",
      "Average loss: 840.131000\n",
      "Learning rate: 0.000019\n",
      "\n",
      "Epoch 18 summary:\n",
      "Average loss: 805.168000\n",
      "Learning rate: 0.000019\n",
      "\n",
      "Epoch 19 summary:\n",
      "Average loss: 771.978000\n",
      "Learning rate: 0.000018\n",
      "\n",
      "Epoch 20 summary:\n",
      "Average loss: 739.162000\n",
      "Learning rate: 0.000018\n",
      "\n",
      "Epoch 21 summary:\n",
      "Average loss: 705.331000\n",
      "Learning rate: 0.000017\n",
      "\n",
      "Epoch 22 summary:\n",
      "Average loss: 674.599000\n",
      "Learning rate: 0.000017\n",
      "\n",
      "Epoch 23 summary:\n",
      "Average loss: 645.600000\n",
      "Learning rate: 0.000016\n",
      "\n",
      "Epoch 24 summary:\n",
      "Average loss: 619.610000\n",
      "Learning rate: 0.000016\n",
      "\n",
      "Epoch 25 summary:\n",
      "Average loss: 592.290000\n",
      "Learning rate: 0.000015\n",
      "\n",
      "Epoch 26 summary:\n",
      "Average loss: 570.694000\n",
      "Learning rate: 0.000014\n",
      "\n",
      "Epoch 27 summary:\n",
      "Average loss: 551.225000\n",
      "Learning rate: 0.000013\n",
      "\n",
      "Epoch 28 summary:\n",
      "Average loss: 534.679000\n",
      "Learning rate: 0.000012\n",
      "\n",
      "Epoch 29 summary:\n",
      "Average loss: 513.641000\n",
      "Learning rate: 0.000012\n",
      "\n",
      "Epoch 30 summary:\n",
      "Average loss: 501.866500\n",
      "Learning rate: 0.000011\n",
      "\n",
      "Epoch 31 summary:\n",
      "Average loss: 488.537500\n",
      "Learning rate: 0.000010\n",
      "\n",
      "Epoch 32 summary:\n",
      "Average loss: 477.751000\n",
      "Learning rate: 0.000009\n",
      "\n",
      "Epoch 33 summary:\n",
      "Average loss: 471.421000\n",
      "Learning rate: 0.000008\n",
      "\n",
      "Epoch 34 summary:\n",
      "Average loss: 455.657500\n",
      "Learning rate: 0.000007\n",
      "\n",
      "Epoch 35 summary:\n",
      "Average loss: 457.987500\n",
      "Learning rate: 0.000007\n",
      "\n",
      "Epoch 36 summary:\n",
      "Average loss: 450.607500\n",
      "Learning rate: 0.000006\n",
      "\n",
      "Epoch 37 summary:\n",
      "Average loss: 438.371500\n",
      "Learning rate: 0.000005\n",
      "\n",
      "Epoch 38 summary:\n",
      "Average loss: 439.828000\n",
      "Learning rate: 0.000004\n",
      "\n",
      "Epoch 39 summary:\n",
      "Average loss: 429.171500\n",
      "Learning rate: 0.000004\n",
      "\n",
      "Epoch 40 summary:\n",
      "Average loss: 433.066500\n",
      "Learning rate: 0.000003\n",
      "\n",
      "Epoch 41 summary:\n",
      "Average loss: 425.999500\n",
      "Learning rate: 0.000002\n",
      "\n",
      "Epoch 42 summary:\n",
      "Average loss: 431.183500\n",
      "Learning rate: 0.000002\n",
      "\n",
      "Epoch 43 summary:\n",
      "Average loss: 426.629000\n",
      "Learning rate: 0.000001\n",
      "\n",
      "Epoch 44 summary:\n",
      "Average loss: 423.533000\n",
      "Learning rate: 0.000001\n",
      "\n",
      "Epoch 45 summary:\n",
      "Average loss: 424.368500\n",
      "Learning rate: 0.000001\n",
      "\n",
      "Epoch 46 summary:\n",
      "Average loss: 419.627500\n",
      "Learning rate: 0.000000\n",
      "\n",
      "Epoch 47 summary:\n",
      "Average loss: 426.159000\n",
      "Learning rate: 0.000000\n",
      "\n",
      "Epoch 48 summary:\n",
      "Average loss: 432.589000\n",
      "Learning rate: 0.000000\n",
      "\n",
      "Epoch 49 summary:\n",
      "Average loss: 426.075000\n",
      "Learning rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 09:24:31,943 - \n",
      "--------------------------------------------------------------------------------\n",
      "2025-10-29 09:24:31,945 - Trial 0 completed\n",
      "2025-10-29 09:24:31,947 - Validation Loss: 416.902966\n",
      "2025-10-29 09:24:31,948 - Duration: 111.48 seconds\n",
      "2025-10-29 09:24:31,949 - --------------------------------------------------------------------------------\n",
      "\n",
      "[I 2025-10-29 09:24:32,098] Trial 0 finished with value: 416.90296630859376 and parameters: {'hidden_channel': 62, 'num_blocks': 4, 'diffusion_steps': 895, 'batch_size': 742, 'start_beta': 0.0003529637220466875, 'end_beta': 0.012562682440227632, 'lr': 1.920476593986205e-05}. Best is trial 0 with value: 416.90296630859376.\n",
      "2025-10-29 09:24:32,101 - \n",
      "Memory Analysis:\n",
      "2025-10-29 09:24:32,102 - Estimated memory requirement: 3.16 GB\n",
      "2025-10-29 09:24:32,103 - Current GPU memory usage: 0.02 GB\n",
      "2025-10-29 09:24:32,104 - \n",
      "================================================================================\n",
      "2025-10-29 09:24:32,105 - Starting Trial 1\n",
      "2025-10-29 09:24:32,106 - Parameters:\n",
      "2025-10-29 09:24:32,107 -   hidden_channel: 63\n",
      "2025-10-29 09:24:32,108 -   num_blocks: 4\n",
      "2025-10-29 09:24:32,109 -   diffusion_steps: 715\n",
      "2025-10-29 09:24:32,110 -   batch_size: 454\n",
      "2025-10-29 09:24:32,111 -   start_beta: 0.00030590882650372253\n",
      "2025-10-29 09:24:32,112 -   end_beta: 0.011089987101073345\n",
      "2025-10-29 09:24:32,113 -   lr: 0.00041100759975580065\n",
      "2025-10-29 09:24:32,114 - ================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 summary:\n",
      "Average loss: 1789.015143\n",
      "Learning rate: 0.000021\n",
      "\n",
      "Epoch 1 summary:\n",
      "Average loss: 1740.267143\n",
      "Learning rate: 0.000034\n",
      "\n",
      "Epoch 2 summary:\n",
      "Average loss: 1656.781714\n",
      "Learning rate: 0.000054\n",
      "\n",
      "Epoch 3 summary:\n",
      "Average loss: 1508.962857\n",
      "Learning rate: 0.000082\n",
      "\n",
      "Epoch 4 summary:\n",
      "Average loss: 1287.540857\n",
      "Learning rate: 0.000116\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from utils import format_time, clear_gpu_memory\n",
    "from prediction import process_single_subject\n",
    "\n",
    "def batch_process_all_subjects(input_dir, output_dir, config_path):\n",
    "    \"\"\"Process all subjects with comprehensive logging and visualization\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all ECoG files\n",
    "    ecog_files = sorted(glob(os.path.join(input_dir, \"*ecog.npy\")))\n",
    "    total_subjects = len(ecog_files)\n",
    "    \n",
    "    print(f\"\\nFound {total_subjects} subjects to process\")\n",
    "    time.sleep(1)  # Give time to read the message\n",
    "    \n",
    "    # Initialize progress tracking\n",
    "    results_summary = []\n",
    "    total_start_time = time.time()\n",
    "    successful_processes = 0\n",
    "    \n",
    "    # Create main progress bar\n",
    "    with tqdm(total=total_subjects, desc=\"Overall Progress\") as pbar:\n",
    "        for subject_idx, ecog_file in enumerate(ecog_files, 1):\n",
    "            # Get corresponding DBS file\n",
    "            dbs_file = ecog_file.replace('ecog.npy', 'dbs.npy')\n",
    "            \n",
    "            if os.path.exists(dbs_file):\n",
    "                # Display progress information\n",
    "                elapsed_time = time.time() - total_start_time\n",
    "                if successful_processes > 0:\n",
    "                    avg_time_per_subject = elapsed_time / successful_processes\n",
    "                    estimated_remaining = avg_time_per_subject * (total_subjects - subject_idx + 1)\n",
    "                    print(f\"\\nEstimated time remaining: {format_time(estimated_remaining)}\")\n",
    "                \n",
    "                print(f\"\\nProcessing subject {subject_idx}/{total_subjects}\")\n",
    "                print(f\"File: {os.path.basename(ecog_file)}\")\n",
    "                \n",
    "                # Process subject\n",
    "                success, duration = process_single_subject(\n",
    "                    ecog_path=ecog_file,\n",
    "                    dbs_path=dbs_file,\n",
    "                    output_dir=output_dir,\n",
    "                    config_path=config_path\n",
    "                )\n",
    "                \n",
    "                if success:\n",
    "                    successful_processes += 1\n",
    "                \n",
    "                # Store results\n",
    "                results_summary.append({\n",
    "                    'subject': os.path.basename(ecog_file),\n",
    "                    'success': success,\n",
    "                    'duration': duration,\n",
    "                    'processed_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Save progress summary\n",
    "                with open(os.path.join(output_dir, 'processing_summary.txt'), 'w') as f:\n",
    "                    f.write(\"Processing Summary:\\n\")\n",
    "                    f.write(f\"Total subjects: {total_subjects}\\n\")\n",
    "                    f.write(f\"Completed: {subject_idx}\\n\")\n",
    "                    f.write(f\"Successful: {successful_processes}\\n\")\n",
    "                    f.write(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "                    \n",
    "                    for result in results_summary:\n",
    "                        f.write(f\"Subject: {result['subject']}\\n\")\n",
    "                        f.write(f\"Success: {result['success']}\\n\")\n",
    "                        f.write(f\"Duration: {format_time(result['duration'])}\\n\")\n",
    "                        f.write(f\"Processed at: {result['processed_at']}\\n\\n\")\n",
    "            else:\n",
    "                print(f\"Warning: No matching DBS file found for {ecog_file}\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Final summary\n",
    "    total_duration = time.time() - total_start_time\n",
    "    print(f\"\\nBatch processing completed!\")\n",
    "    print(f\"Total time: {format_time(total_duration)}\")\n",
    "    print(f\"Successfully processed: {successful_processes}/{total_subjects}\")\n",
    "    \n",
    "    # Create PSD figure directory\n",
    "    psd_dir = os.path.join(output_dir, 'psd_figures')\n",
    "    os.makedirs(psd_dir, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    input_dir = r'C:\\Users\\admin\\1代码 通过ecog数据预测stn的beta\\data_upload\\demo dataset\\ddpm'\n",
    "    output_dir = r'C:\\Users\\admin\\1代码 通过ecog数据预测stn的beta\\data_upload\\demo dataset\\ddpm_output'\n",
    "    config_path = \"../conf\"\n",
    "    \n",
    "    # Print system info\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Initial GPU memory allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "    \n",
    "    # Process all subjects\n",
    "    batch_process_all_subjects(input_dir, output_dir, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb18e6a-09a3-4147-9046-f7d82aa32968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1e858-4f77-4fd0-a21e-0b279bf4f23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4caa8-88ed-48f9-a9e3-9fa6d49fb0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27d965-be93-4472-a366-9ea6cb4f3ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb49dabe-8c60-44eb-9a1f-ccdcfec46687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a133a-f327-482d-b081-6991936d3bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158946c1-7cc6-4d2d-ac56-db975ccc8a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba1015-2e50-46d7-93e8-8ac039471dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
