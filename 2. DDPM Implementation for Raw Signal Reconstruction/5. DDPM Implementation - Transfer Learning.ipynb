{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb860b61-8c55-4e68-ab56-a299efe4dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "from hydra import initialize, compose\n",
    "from ntd.diffusion_model import Diffusion\n",
    "from ntd.networks import AdaConv\n",
    "from ntd.utils.kernels_and_diffusion_utils import OUProcess\n",
    "from trainer import train_ecog_dbs_model\n",
    "from prediction import get_all_predictions_fast_simple\n",
    "from utils import clear_gpu_memory\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "class ECoGDBSDataset(Dataset):\n",
    "    def __init__(self, ecog_data, dbs_data, augment=False, augment_params=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ecog_data: shape (N, 3, 1000)\n",
    "            dbs_data: shape (N, 1, 1000)\n",
    "            augment: whether to apply data augmentation\n",
    "            augment_params: dictionary containing augmentation parameters\n",
    "        \"\"\"\n",
    "        self.ecog_data = torch.tensor(ecog_data, dtype=torch.float32)\n",
    "        self.dbs_data = torch.tensor(dbs_data, dtype=torch.float32)\n",
    "        self.augment = augment\n",
    "        self.augment_params = augment_params or {\n",
    "            'noise_std': 0.01,\n",
    "            'max_shift': 20,\n",
    "            'dropout_prob': 0.1\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ecog_data)\n",
    "    \n",
    "    def apply_augmentation(self, ecog, dbs):\n",
    "        # Random noise\n",
    "        if torch.rand(1) < 0.5:\n",
    "            ecog = ecog + torch.randn_like(ecog) * self.augment_params['noise_std']\n",
    "        \n",
    "        # Random time shift\n",
    "        if torch.rand(1) < 0.5:\n",
    "            shift = np.random.randint(-self.augment_params['max_shift'], \n",
    "                                    self.augment_params['max_shift'])\n",
    "            ecog = torch.roll(ecog, shift, dims=-1)\n",
    "            dbs = torch.roll(dbs, shift, dims=-1)\n",
    "        \n",
    "        # Random channel dropout for ECoG\n",
    "        if torch.rand(1) < self.augment_params['dropout_prob']:\n",
    "            channel = np.random.randint(0, ecog.shape[0])\n",
    "            ecog[channel] = ecog[channel] * (torch.rand_like(ecog[channel]) > 0.1).float()\n",
    "        \n",
    "        return ecog, dbs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ecog = self.ecog_data[idx]\n",
    "        dbs = self.dbs_data[idx]\n",
    "        \n",
    "        if self.augment:\n",
    "            ecog, dbs = self.apply_augmentation(ecog, dbs)\n",
    "            \n",
    "        return {\n",
    "            'cond': ecog,\n",
    "            'signal': dbs\n",
    "        }\n",
    "\n",
    "def train_base_model(train_dataset, val_dataset, config):\n",
    "    \"\"\"Train the base model with the initial dataset using existing training function\"\"\"\n",
    "    print(\"Training base model...\")\n",
    "    diffusion_model, _, _ = train_ecog_dbs_model(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        config=config\n",
    "    )\n",
    "    return diffusion_model\n",
    "\n",
    "class TransferLearningTrainer:\n",
    "    def __init__(self, base_model, config, device='cuda'):\n",
    "        self.base_model = base_model.to(device)\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.lambda_reg = 0.01\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    def compute_regularization_loss(self, current_model):\n",
    "        \"\"\"Compute L2 regularization loss towards base model weights\"\"\"\n",
    "        reg_loss = 0\n",
    "        for (name, param), (_, param_base) in zip(\n",
    "            current_model.named_parameters(), \n",
    "            self.base_model.named_parameters()\n",
    "        ):\n",
    "            reg_loss += torch.nn.functional.mse_loss(param, param_base.detach())\n",
    "        return reg_loss * self.lambda_reg\n",
    "    \n",
    "    def train_one_epoch(self, model, train_loader, optimizer, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch}') as pbar:\n",
    "            for batch in pbar:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Move batch to device\n",
    "                sig_batch = batch[\"signal\"].to(self.device, non_blocking=True)\n",
    "                cond_batch = batch[\"cond\"].to(self.device, non_blocking=True)\n",
    "                \n",
    "                # Forward pass using train_batch instead of get_loss\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss = model.train_batch(sig_batch, cond=cond_batch)\n",
    "                    loss = torch.mean(loss)  # Average the loss\n",
    "                    \n",
    "                    # Add regularization loss\n",
    "                    reg_loss = self.compute_regularization_loss(model)\n",
    "                    total_loss = loss + reg_loss\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                self.scaler.scale(total_loss).backward()\n",
    "                self.scaler.step(optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                pbar.set_postfix({'loss': total_loss.item()})\n",
    "                batch_count += 1\n",
    "                \n",
    "                if batch_count % 50 == 0:\n",
    "                    clear_gpu_memory()\n",
    "                \n",
    "        return total_loss / batch_count\n",
    "    \n",
    "    def validate(self, model, val_loader):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sig_batch = batch[\"signal\"].to(self.device, non_blocking=True)\n",
    "                cond_batch = batch[\"cond\"].to(self.device, non_blocking=True)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss = model.train_batch(sig_batch, cond=cond_batch)\n",
    "                    loss = torch.mean(loss)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "        return total_loss / batch_count\n",
    "    \n",
    "    def gradual_unfreeze_finetune(self, model, train_loader, val_loader, finetune_config):\n",
    "        \"\"\"Implement gradual unfreezing during fine-tuning\"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        \n",
    "        # First freeze all blocks\n",
    "        for param in model.network.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Gradually unfreeze blocks from last to first\n",
    "        num_blocks = len(model.network.blocks)\n",
    "        \n",
    "        for block_idx in range(num_blocks-1, -1, -1):\n",
    "            print(f\"\\nUnfreezing block {block_idx}\")\n",
    "            \n",
    "            # Unfreeze current block\n",
    "            for param in model.network.blocks[block_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "            # Create optimizer and scheduler\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=finetune_config.optimizer.lr,\n",
    "                weight_decay=finetune_config.optimizer.weight_decay\n",
    "            )\n",
    "            \n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=finetune_config.optimizer.lr,\n",
    "                epochs=finetune_config.optimizer.num_epochs_per_block,\n",
    "                steps_per_epoch=len(train_loader)\n",
    "            )\n",
    "            \n",
    "            # Train for specified epochs with current frozen state\n",
    "            for epoch in range(finetune_config.optimizer.num_epochs_per_block):\n",
    "                train_loss = self.train_one_epoch(model, train_loader, optimizer, epoch)\n",
    "                val_loss = self.validate(model, val_loader)\n",
    "                scheduler.step()\n",
    "                \n",
    "                print(f\"Block {block_idx}, Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "                \n",
    "                # Save best model\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "        \n",
    "        # Load best model state\n",
    "        model.load_state_dict(best_model_state)\n",
    "        return model, best_val_loss\n",
    "\n",
    "def finetune_and_evaluate(base_model, new_subject_data, new_subject_labels, output_dir, new_sub_name, config,\n",
    "                         finetune_epoch, batch_size):\n",
    "    \"\"\"Main function for fine-tuning and evaluation\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create splits\n",
    "    n_samples = len(new_subject_data)\n",
    "    train_size = int(0.6 * n_samples)\n",
    "    val_size = int(0.2 * n_samples)\n",
    "    test_size = n_samples - train_size - val_size\n",
    "    \n",
    "    # Create datasets with augmentation for training\n",
    "    train_dataset = ECoGDBSDataset(\n",
    "        new_subject_data[:train_size], \n",
    "        new_subject_labels[:train_size],\n",
    "        augment=True\n",
    "    )\n",
    "    val_dataset = ECoGDBSDataset(\n",
    "        new_subject_data[train_size:train_size+val_size],\n",
    "        new_subject_labels[train_size:train_size+val_size]\n",
    "    )\n",
    "    test_dataset = ECoGDBSDataset(\n",
    "        new_subject_data[train_size+val_size:],\n",
    "        new_subject_labels[train_size+val_size:]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.optimizer.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.optimizer.train_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Modify config for fine-tuning\n",
    "    finetune_config = deepcopy(config)\n",
    "    finetune_config.optimizer.lr *= 0.1\n",
    "    finetune_config.optimizer.warmup_epochs = 5\n",
    "    finetune_config.optimizer.num_epochs_per_block = 20\n",
    "    finetune_config.optimizer.num_epochs = finetune_epoch\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = TransferLearningTrainer(base_model, finetune_config, device)\n",
    "    \n",
    "    # Fine-tune with gradual unfreezing\n",
    "    finetuned_model, best_val_loss = trainer.gradual_unfreeze_finetune(\n",
    "        deepcopy(base_model),\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        finetune_config\n",
    "    )\n",
    "    \n",
    "    # Generate predictions on test set\n",
    "    print(\"Generating predictions...\")\n",
    "    results = get_all_predictions_fast_simple(finetuned_model, test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Save model and results\n",
    "    save_path = os.path.join(output_dir, f\"finetuned_model_{new_sub_name}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': finetuned_model.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': finetune_config\n",
    "    }, save_path)\n",
    "    \n",
    "    results_path = os.path.join(output_dir, f\"predicted_results_{new_sub_name}.npy\")\n",
    "    np.save(results_path, np.stack([results['real_dbs'], results['imputed_dbs']]))\n",
    "    \n",
    "    return results, finetuned_model\n",
    "    \n",
    "def load_subject_data(subject_id, data_dir):\n",
    "    \"\"\"Load DBS and ECoG data for a single subject\"\"\"\n",
    "    dbs_path = os.path.join(data_dir, f'{subject_id}_dbs.npy')\n",
    "    ecog_path = os.path.join(data_dir, f'{subject_id}_ecog.npy')\n",
    "    \n",
    "    dbs_data = np.load(dbs_path).astype(np.float32)  # Shape: (n_samples, 1, 1000)\n",
    "    ecog_data = np.load(ecog_path).astype(np.float32)  # Shape: (n_samples, 3, 1000)\n",
    "    \n",
    "    return dbs_data, ecog_data\n",
    "\n",
    "def create_train_test_split(test_subject, data_dir):\n",
    "    \"\"\"Create training and testing datasets using leave-one-subject-out\"\"\"\n",
    "    # Initialize empty lists for training data\n",
    "    train_dbs = []\n",
    "    train_ecog = []\n",
    "    \n",
    "    # Load test subject data\n",
    "    test_dbs, test_ecog = load_subject_data(test_subject, data_dir)\n",
    "    \n",
    "    # Load all other subjects' data for training\n",
    "    for subject_id in subject_ids:\n",
    "        if subject_id != test_subject:\n",
    "            dbs_data, ecog_data = load_subject_data(subject_id, data_dir)\n",
    "            train_dbs.append(dbs_data)\n",
    "            train_ecog.append(ecog_data)\n",
    "    \n",
    "    # Concatenate all training data\n",
    "    train_dbs = np.concatenate(train_dbs, axis=0)\n",
    "    train_ecog = np.concatenate(train_ecog, axis=0)\n",
    "    \n",
    "    return (train_dbs, train_ecog), (test_dbs, test_ecog)\n",
    "    #return (train_dbs[:2000,:,:], train_ecog[:2000,:,:]), (test_dbs[:2000,:,:], test_ecog[:2000,:,:])\n",
    "\n",
    "def create_model(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize model components\n",
    "    network = AdaConv(\n",
    "        signal_length=config.dataset.signal_length,\n",
    "        signal_channel=config.network.signal_channel,\n",
    "        cond_dim=config.network.cond_dim,\n",
    "        hidden_channel=config.network.hidden_channel,\n",
    "        in_kernel_size=config.network.in_kernel_size,\n",
    "        out_kernel_size=config.network.out_kernel_size,\n",
    "        slconv_kernel_size=config.network.slconv_kernel_size,\n",
    "        num_scales=config.network.num_scales,\n",
    "        num_blocks=config.network.num_blocks,\n",
    "        num_off_diag=config.network.num_off_diag,\n",
    "        use_pos_emb=config.network.use_pos_emb,\n",
    "        padding_mode=config.network.padding_mode,\n",
    "        use_fft_conv=config.network.use_fft_conv,\n",
    "    ).to(device)\n",
    "    \n",
    "    ou_process = OUProcess(\n",
    "        config.diffusion_kernel.sigma_squared,\n",
    "        config.diffusion_kernel.ell,\n",
    "        config.dataset.signal_length\n",
    "    ).to(device)\n",
    "    \n",
    "    diffusion = Diffusion(\n",
    "        network=network,\n",
    "        noise_sampler=ou_process,\n",
    "        mal_dist_computer=ou_process,\n",
    "        diffusion_time_steps=config.diffusion.diffusion_steps,\n",
    "        schedule=config.diffusion.schedule,\n",
    "        start_beta=config.diffusion.start_beta,\n",
    "        end_beta=config.diffusion.end_beta,\n",
    "    ).to(device)\n",
    "    \n",
    "    return diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3c601-0784-4774-95f8-4dfcf2367d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_characteristics(ecog_data, dbs_data, subject_id=None):\n",
    "    \"\"\"\n",
    "    Analyze characteristics of ECoG and DBS data to inform augmentation parameters.\n",
    "    \n",
    "    Args:\n",
    "        ecog_data: shape (N, 3, 1000)\n",
    "        dbs_data: shape (N, 1, 1000)\n",
    "        subject_id: optional subject identifier for plotting\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    results['ecog_mean'] = np.mean(ecog_data, axis=(0, 2))\n",
    "    results['ecog_std'] = np.std(ecog_data, axis=(0, 2))\n",
    "    results['dbs_mean'] = np.mean(dbs_data, axis=(0, 2))\n",
    "    results['dbs_std'] = np.std(dbs_data, axis=(0, 2))\n",
    "    \n",
    "    # Compute signal ranges\n",
    "    results['ecog_range'] = np.percentile(ecog_data, [1, 99], axis=(0, 2))\n",
    "    results['dbs_range'] = np.percentile(dbs_data, [1, 99], axis=(0, 2))\n",
    "    \n",
    "    # Temporal characteristics\n",
    "    results['temporal_stats'] = {}\n",
    "    \n",
    "    # Compute average autocorrelation for each channel\n",
    "    for i in range(ecog_data.shape[1]):\n",
    "        autocorr = np.array([np.correlate(ecog_data[j, i], ecog_data[j, i], mode='full') \n",
    "                            for j in range(min(1000, ecog_data.shape[0]))])\n",
    "        results['temporal_stats'][f'ecog_ch{i}_autocorr'] = np.mean(autocorr, axis=0)\n",
    "    \n",
    "    autocorr_dbs = np.array([np.correlate(dbs_data[j, 0], dbs_data[j, 0], mode='full') \n",
    "                            for j in range(min(1000, dbs_data.shape[0]))])\n",
    "    results['temporal_stats']['dbs_autocorr'] = np.mean(autocorr_dbs, axis=0)\n",
    "    \n",
    "    # Cross-correlation between channels\n",
    "    results['cross_corr'] = np.zeros((3, 3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i != j:\n",
    "                corr = np.array([np.corrcoef(ecog_data[k, i], ecog_data[k, j])[0, 1] \n",
    "                               for k in range(ecog_data.shape[0])])\n",
    "                results['cross_corr'][i, j] = np.mean(corr)    \n",
    "    return results\n",
    "\n",
    "def suggest_augmentation_params(results):\n",
    "    \"\"\"\n",
    "    Suggest augmentation parameters based on data characteristics.\n",
    "    \"\"\"\n",
    "    suggestions = {}\n",
    "    \n",
    "    # Noise level based on signal std\n",
    "    ecog_noise_std = np.mean(results['ecog_std']) * 0.1\n",
    "    suggestions['noise_std'] = float(ecog_noise_std)\n",
    "    \n",
    "    # Max shift based on autocorrelation\n",
    "    autocorr_ecog = np.mean([results['temporal_stats'][f'ecog_ch{i}_autocorr'] \n",
    "                            for i in range(3)], axis=0)\n",
    "    lag_threshold = 0.5  # correlation threshold\n",
    "    max_lag = np.where(autocorr_ecog < lag_threshold * autocorr_ecog.max())[0]\n",
    "    if len(max_lag) > 0:\n",
    "        suggestions['max_shift'] = int(max_lag[0])\n",
    "    else:\n",
    "        suggestions['max_shift'] = 20  # default value\n",
    "    \n",
    "    # Dropout probability based on cross-correlations\n",
    "    mean_cross_corr = np.mean(np.abs(results['cross_corr']))\n",
    "    suggestions['dropout_prob'] = float(min(0.2, 1 - mean_cross_corr))\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "def analyze_subject_data(data_dir, subject_id):\n",
    "    \"\"\"\n",
    "    Load and analyze data for a specific subject.\n",
    "    \"\"\"\n",
    "    dbs_data, ecog_data = load_subject_data(subject_id, data_dir)\n",
    "    results = analyze_data_characteristics(ecog_data, dbs_data, subject_id)\n",
    "    suggestions = suggest_augmentation_params(results)\n",
    "    \n",
    "    print(f\"\\nSuggested augmentation parameters for subject {subject_id}:\")\n",
    "    print(f\"noise_std: {suggestions['noise_std']:.4f}\")\n",
    "    print(f\"max_shift: {suggestions['max_shift']}\")\n",
    "    print(f\"dropout_prob: {suggestions['dropout_prob']:.4f}\")\n",
    "    \n",
    "    return results, suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0837f-a442-4944-afe7-72b6f62bae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "data_dir = r'E:\\data_zixiao\\uscf_npy_3d_4s_nor_rmbad_9'\n",
    "\n",
    "# Get all subject IDs by looking at the dbs files\n",
    "dbs_files = sorted([f for f in os.listdir(data_dir) if f.endswith('_dbs.npy')])\n",
    "subject_ids = [f.split('_dbs.npy')[0] for f in dbs_files]\n",
    "\n",
    "# Initialize config\n",
    "with initialize(version_base=None, config_path = \"../ecog_stn_icnworkstation/conf\"):\n",
    "    config = compose(config_name=\"62_config_ou_200_more_complex_transfer\")\n",
    "output_dir=r\"E:\\data_zixiao\\raw_prediction_62_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9ae4b-c6a6-45bd-a71e-eeedc7a7577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_subject in tqdm(subject_ids):\n",
    "    print(f\"\\nProcessing subject: {test_subject}\")\n",
    "    \n",
    "    # Get train and test data\n",
    "    (dbs_data, ecog_data), (new_subject_dbs, new_subject_ecog) = create_train_test_split(\n",
    "        test_subject, data_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"Training data shapes:\")\n",
    "    print(f\"DBS: {dbs_data.shape}, ECoG: {ecog_data.shape}\")\n",
    "    print(f\"Testing data shapes:\")\n",
    "    print(f\"DBS: {new_subject_dbs.shape}, ECoG: {new_subject_ecog.shape}\")\n",
    "\n",
    "    results, suggestions = analyze_subject_data(data_dir, test_subject)\n",
    "\n",
    "    # Update config with suggested parameters\n",
    "    config.dataset.augmentation.noise_std = suggestions['noise_std']\n",
    "    config.dataset.augmentation.max_shift = suggestions['max_shift']\n",
    "    config.dataset.augmentation.dropout_prob = suggestions['dropout_prob']\n",
    "    \n",
    "    \n",
    "    # Create full dataset and split for base model training\n",
    "    full_dataset = ECoGDBSDataset(ecog_data, dbs_data)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, len(full_dataset) - train_size]\n",
    "    )\n",
    "    # Train base model\n",
    "    base_model = train_base_model(train_dataset, val_dataset, config)\n",
    "    \n",
    "    # Finetune and evaluate\n",
    "    results, finetuned_model = finetune_and_evaluate(\n",
    "        base_model,\n",
    "        new_subject_ecog,\n",
    "        new_subject_dbs,\n",
    "        output_dir=output_dir,\n",
    "        config=config,\n",
    "        new_sub_name=test_subject,\n",
    "        finetune_epoch=80,\n",
    "        batch_size=1024\n",
    "    )\n",
    "    \n",
    "    # Clear memory\n",
    "    del dbs_data, ecog_data, new_subject_dbs, new_subject_ecog\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
