{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd9f87-9a40-4a89-9d01-5b1bef1e78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr, zscore\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.fft as fft\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import Sampler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from math import ceil\n",
    "def apply_hilbert_torch(x, envelope=False, do_log=False, compute_val='power', data_srate=250):\n",
    "    def hilbert_torch(x):\n",
    "        N = x.size(-1)\n",
    "        Xf = fft.fft(x, dim=-1)\n",
    "        h = torch.zeros(N, dtype=torch.complex64, device=x.device)\n",
    "        if N % 2 == 0:\n",
    "            h[0] = h[N // 2] = 1\n",
    "            h[1:N // 2] = 2\n",
    "        else:\n",
    "            h[0] = 1\n",
    "            h[1:(N + 1) // 2] = 2\n",
    "        return fft.ifft(Xf * h, dim=-1)\n",
    "    def angle_custom(z):\n",
    "        return torch.atan2(z.imag, z.real)\n",
    "    def unwrap(p, discont=np.pi):\n",
    "        dp = p[..., 1:] - p[..., :-1]\n",
    "        ddp = torch.remainder(dp + np.pi, 2 * np.pi) - np.pi\n",
    "        ddp[torch.abs(dp) < discont] = 0\n",
    "        p_unwrapped = p.clone()\n",
    "        p_unwrapped[..., 1:] = p[..., 0][..., None] + torch.cumsum(dp + ddp, dim=-1)\n",
    "        return p_unwrapped\n",
    "    def diff(x):\n",
    "        return x[..., 1:] - x[..., :-1]\n",
    "    n_x = x.size(-1)\n",
    "    hilb_sig = hilbert_torch(x)\n",
    "    \n",
    "    if compute_val == 'power':\n",
    "        out = torch.abs(hilb_sig)\n",
    "        if do_log:\n",
    "            out = torch.log1p(out)\n",
    "    elif compute_val == 'phase':\n",
    "        out = unwrap(angle_custom(hilb_sig))\n",
    "    elif compute_val == 'freqslide':\n",
    "        ang = angle_custom(hilb_sig)\n",
    "        ang = data_srate * diff(unwrap(ang)) / (2 * np.pi)\n",
    "        out = torch.nn.functional.pad(ang, (0, 1), mode='constant')\n",
    "        # TO DO: apply median filter (use torch.median or a custom implementation)\n",
    "    return out\n",
    "\n",
    "# get gpu device\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de13ec-beff-4b6b-bfd4-3485d0bf6559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtxNet(nn.Module):\n",
    "    def __init__(self, Chans=3, Samples=375, dropoutRate=0.65, kernLength=64, F1=4, \n",
    "                 D=2, F2=8, norm_rate=0.25, kernLength_sep=16,\n",
    "                 do_log=False, data_srate=1, base_split=4):\n",
    "        super(CtxNet, self).__init__()\n",
    "        self.do_log = do_log\n",
    "        self.data_srate = data_srate\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.Conv2d(F1, F1*D, (Chans, 1), groups=F1, bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F1*D),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(F1*D, F2, (1, kernLength_sep), bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        flatten_size = self.calculate_flatten_size(Chans, Samples, F2)\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(flatten_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, 1)  # Single output as we're predicting one band at a time\n",
    "\n",
    "    def calculate_flatten_size(self, Chans, Samples, F2):\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, 1, Chans, Samples)\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.block1(x)\n",
    "        x = self.apply_hilbert(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def apply_hilbert(self, x):\n",
    "        return apply_hilbert_torch(x, do_log=self.do_log, compute_val='power', data_srate=self.data_srate)\n",
    "\n",
    "def create_model(Chans, Samples=375, dropoutRate=0.65, kernLength=64, F1=4, D=2, F2=8):\n",
    "    model = CtxNet(Chans=Chans, Samples=Samples, dropoutRate=dropoutRate, \n",
    "                 kernLength=kernLength, F1=F1, D=D, F2=F2)\n",
    "    return model\n",
    "\n",
    "def positive_correlation_loss(y_true, y_pred, epsilon=1e-8):\n",
    "    mx = torch.mean(y_pred)\n",
    "    my = torch.mean(y_true)\n",
    "    xm, ym = y_pred - mx, y_true - my\n",
    "    # Check if we have enough elements to calculate std\n",
    "    if xm.numel() > 1 and ym.numel() > 1:\n",
    "        r_num = torch.mean(xm * ym)\n",
    "        r_den = torch.std(xm) * torch.std(ym)\n",
    "        r = r_num / (r_den + epsilon)\n",
    "    else:\n",
    "        # If we don't have enough elements, return a default loss\n",
    "        return torch.tensor(1.0, device=y_true.device)\n",
    "    \n",
    "    loss = torch.where(r >= 0, 1 - r, 2 - r)\n",
    "    return loss\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience=7):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7)\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def objective(params, X, y, n_splits=4):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "    scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        train_dataset = CustomDataset(X_train, y_train)\n",
    "        val_dataset = CustomDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        model_params = {\n",
    "            'Chans': X.shape[1],\n",
    "            'dropoutRate': params['dropoutRate'],\n",
    "            'kernLength': params['kernLength'],\n",
    "            'F1': params['F1'],\n",
    "            'D': params['D'],\n",
    "            'F2': params['F2']\n",
    "        }\n",
    "        model = create_model(**model_params)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], \n",
    "                                     weight_decay=params['weight_decay'])\n",
    "        criterion = positive_correlation_loss\n",
    "        \n",
    "        trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                    num_epochs=params['max_epochs'], patience=params['patience'])\n",
    "\n",
    "        trained_model.eval()\n",
    "        device = next(trained_model.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = []\n",
    "            for batch in val_loader:\n",
    "                inputs, _ = batch\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = trained_model(inputs)\n",
    "                y_val_pred.extend(outputs.cpu().numpy())\n",
    "            y_val_pred = np.array(y_val_pred).squeeze()\n",
    "        \n",
    "        score = positive_correlation_loss(torch.from_numpy(y_val), torch.from_numpy(y_val_pred)).numpy().item()\n",
    "        scores.append(score)\n",
    "\n",
    "        # Clear memory\n",
    "        del model, trained_model, optimizer, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bb9e5-4eec-4e9e-85e7-71c5c3e7ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'E:\\data_zixiao\\ucsf_list.xlsx',\n",
    "                       index_col = [0])\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01324810-3e75-48db-854d-b54c4ce770c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "training_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "pearsonr_value = []\n",
    "target_list = []\n",
    "file_list = []\n",
    "train_size_list = []\n",
    "\n",
    "batch_size = 8192\n",
    "f_folder = r'E:\\data_zixiao\\uscf_npy_3d_1.5s_1hz_ctxnet'\n",
    "save_folder = r'E:\\data_zixiao\\raw_prediction_60'\n",
    "\n",
    "space = [\n",
    "    Integer(32, 128, name='kernLength'),\n",
    "    Integer(2, 8, name='F1'),\n",
    "    Integer(1, 8, name='D'),\n",
    "    Integer(4, 16, name='F2'),\n",
    "    Real(0.3, 0.8, name='dropoutRate'),\n",
    "    Categorical([1e-3], name='learning_rate'),\n",
    "    Real(1e-6, 1e-3, \"log-uniform\", name='weight_decay'),\n",
    "    Integer(3, 6, name='patience'),\n",
    "    Categorical([30], name='max_epochs')\n",
    "]\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    y_name = df['name'][i][:-4]+'_tarstn.npy'\n",
    "    y_all = np.load(f'{f_folder}/{y_name}')\n",
    "    x_name = df['name'][i][:-4]+'_ecog.npy'\n",
    "    x_all = np.load(f'{f_folder}/{x_name}')\n",
    "    x_all = x_all.reshape((x_all.shape[0], x_all.shape[1], x_all.shape[2]))\n",
    "    \n",
    "    lowbeta = y_all[:,0].squeeze()\n",
    "    highbeta = y_all[:,1].squeeze()\n",
    "    alpha = y_all[:,2].squeeze()\n",
    "    gamma = y_all[:,3].squeeze()\n",
    "    data = x_all\n",
    "\n",
    "    # First create train/test split for X\n",
    "    X_full_train, X_test = train_test_split(\n",
    "        data,\n",
    "        test_size=0.3,\n",
    "        shuffle=False,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Then split each y variable separately but with the same random state\n",
    "    y_full_train_dict = {}\n",
    "    y_test_dict = {}\n",
    "    for name, y_data in {'highbeta': highbeta, 'lowbeta': lowbeta, 'alpha': alpha, 'gamma': gamma}.items():\n",
    "        y_train, y_test = train_test_split(\n",
    "            y_data,\n",
    "            test_size=0.3,\n",
    "            shuffle=False,\n",
    "            random_state=42\n",
    "        )\n",
    "        y_full_train_dict[name] = y_train\n",
    "        y_test_dict[name] = y_test\n",
    "\n",
    "    for beta in ['highbeta', 'lowbeta', 'alpha', 'gamma']:\n",
    "        targets = locals()[beta]  # Get the full dataset targets\n",
    "        y_full_train = y_full_train_dict[beta]\n",
    "        y_test = y_test_dict[beta]\n",
    "\n",
    "        for train_size in training_sizes:\n",
    "            if train_size < 1.0:\n",
    "                # Take a subset of the full training data\n",
    "                X_train_val, _, y_train_val, _ = train_test_split(\n",
    "                    X_full_train,\n",
    "                    y_full_train,\n",
    "                    train_size=train_size,\n",
    "                    shuffle=False,\n",
    "                    random_state=42\n",
    "                )\n",
    "            else:\n",
    "                # Use full training set\n",
    "                X_train_val = X_full_train\n",
    "                y_train_val = y_full_train\n",
    "\n",
    "            # Scale the features\n",
    "            scaler_X = MinMaxScaler()\n",
    "            scaler_y = MinMaxScaler()\n",
    "\n",
    "            X_train_val_reshaped = X_train_val.reshape(-1, X_train_val.shape[-1])\n",
    "            X_train_val_normalized = scaler_X.fit_transform(X_train_val_reshaped)\n",
    "            X_train_val_normalized = X_train_val_normalized.reshape(X_train_val.shape)\n",
    "\n",
    "            y_train_val_normalized = scaler_y.fit_transform(y_train_val.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Optimize hyperparameters\n",
    "            res_gp = gp_minimize(\n",
    "                lambda params: objective(\n",
    "                    dict(zip([dim.name for dim in space], params)),\n",
    "                    X_train_val_normalized,\n",
    "                    y_train_val_normalized\n",
    "                ),\n",
    "                space,\n",
    "                n_calls=5,  # Changed from 10 to 5\n",
    "                n_initial_points=3,  # Added this to ensure we have enough initial points\n",
    "                random_state=42\n",
    "            )\n",
    "            best_params = dict(zip([dim.name for dim in space], res_gp.x))\n",
    "\n",
    "            # Train final model with best parameters\n",
    "            model_params = {\n",
    "                'Chans': data.shape[1],\n",
    "                'dropoutRate': best_params['dropoutRate'],\n",
    "                'kernLength': best_params['kernLength'],\n",
    "                'F1': best_params['F1'],\n",
    "                'D': best_params['D'],\n",
    "                'F2': best_params['F2']\n",
    "            }\n",
    "            best_model = create_model(**model_params)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(\n",
    "                best_model.parameters(),\n",
    "                lr=best_params['learning_rate'],\n",
    "                weight_decay=best_params['weight_decay']\n",
    "            )\n",
    "            criterion = positive_correlation_loss\n",
    "\n",
    "            train_dataset = CustomDataset(X_train_val_normalized, y_train_val_normalized)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "            # Train the model\n",
    "            best_model = train_model(\n",
    "                best_model,\n",
    "                train_loader,\n",
    "                train_loader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                num_epochs=best_params['max_epochs'],\n",
    "                patience=best_params['patience']\n",
    "            )\n",
    "\n",
    "            # Evaluate on test set and full dataset\n",
    "            best_model.eval()\n",
    "            device = next(best_model.parameters()).device\n",
    "            with torch.no_grad():\n",
    "                # Predict for test set\n",
    "                X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "                X_test_normalized = scaler_X.transform(X_test_reshaped)\n",
    "                X_test_normalized = X_test_normalized.reshape(X_test.shape)\n",
    "                \n",
    "                y_pred_test_normalized = []\n",
    "                for batch in DataLoader(\n",
    "                    CustomDataset(X_test_normalized, np.zeros(len(X_test_normalized))),\n",
    "                    batch_size=batch_size\n",
    "                ):\n",
    "                    inputs, _ = batch\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = best_model(inputs)\n",
    "                    y_pred_test_normalized.extend(outputs.cpu().numpy())\n",
    "                \n",
    "                y_pred_test_normalized = np.array(y_pred_test_normalized)\n",
    "                y_pred_test = scaler_y.inverse_transform(y_pred_test_normalized.reshape(-1, 1)).flatten()\n",
    "\n",
    "                # Save y_test and y_pred_test in a single 2D .npy file\n",
    "                base_filename = os.path.splitext(y_name)[0]\n",
    "                y_test_combined = np.column_stack((y_test, y_pred_test))\n",
    "                np.save(f'{save_folder}/{base_filename}_{beta}_train{int(train_size*100)}_y_test.npy', \n",
    "                       y_test_combined)\n",
    "\n",
    "                # Predict for full dataset\n",
    "                X_all_reshaped = x_all.reshape(-1, x_all.shape[-1])\n",
    "                X_all_normalized = scaler_X.transform(X_all_reshaped)\n",
    "                X_all_normalized = X_all_normalized.reshape(x_all.shape)\n",
    "                \n",
    "                y_pred_all_normalized = []\n",
    "                for batch in DataLoader(\n",
    "                    CustomDataset(X_all_normalized, np.zeros(len(X_all_normalized))),\n",
    "                    batch_size=batch_size\n",
    "                ):\n",
    "                    inputs, _ = batch\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = best_model(inputs)\n",
    "                    y_pred_all_normalized.extend(outputs.cpu().numpy())\n",
    "                \n",
    "                y_pred_all_normalized = np.array(y_pred_all_normalized)\n",
    "                y_pred_all = scaler_y.inverse_transform(y_pred_all_normalized.reshape(-1, 1)).flatten()\n",
    "\n",
    "                # Save y_all and y_pred_all in a single 2D .npy file\n",
    "                y_all_combined = np.column_stack((targets, y_pred_all))\n",
    "                np.save(f'{save_folder}/{base_filename}_{beta}_train{int(train_size*100)}_y_all.npy', \n",
    "                       y_all_combined)\n",
    "\n",
    "                # Store results\n",
    "                pearsonr_value.append(pearsonr(y_pred_test, y_test)[0])\n",
    "                target_list.append(beta)\n",
    "                file_list.append(df['name'][i])\n",
    "                train_size_list.append(train_size)\n",
    "\n",
    "            # Clear memory\n",
    "            del best_model, optimizer, train_loader\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Save results after each frequency band\n",
    "        results = pd.DataFrame({\n",
    "            'value': pearsonr_value,\n",
    "            'predict': target_list,\n",
    "            'f_name': file_list,\n",
    "            'train_size': train_size_list\n",
    "        })\n",
    "        results.to_excel(r'E:\\data_zixiao\\results_ucsf_1hz_training_size_analysis.xlsx')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
