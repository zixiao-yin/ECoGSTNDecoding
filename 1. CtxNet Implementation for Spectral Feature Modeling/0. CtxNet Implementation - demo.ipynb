{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38ecbe9-b015-4971-9cf5-6d594cc82e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr, zscore\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.fft as fft\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import Sampler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from math import ceil\n",
    "def apply_hilbert_torch(x, envelope=False, do_log=False, compute_val='power', data_srate=250):\n",
    "    def hilbert_torch(x):\n",
    "        N = x.size(-1)\n",
    "        Xf = fft.fft(x, dim=-1)\n",
    "        h = torch.zeros(N, dtype=torch.complex64, device=x.device)\n",
    "        if N % 2 == 0:\n",
    "            h[0] = h[N // 2] = 1\n",
    "            h[1:N // 2] = 2\n",
    "        else:\n",
    "            h[0] = 1\n",
    "            h[1:(N + 1) // 2] = 2\n",
    "        return fft.ifft(Xf * h, dim=-1)\n",
    "    def angle_custom(z):\n",
    "        return torch.atan2(z.imag, z.real)\n",
    "    def unwrap(p, discont=np.pi):\n",
    "        dp = p[..., 1:] - p[..., :-1]\n",
    "        ddp = torch.remainder(dp + np.pi, 2 * np.pi) - np.pi\n",
    "        ddp[torch.abs(dp) < discont] = 0\n",
    "        p_unwrapped = p.clone()\n",
    "        p_unwrapped[..., 1:] = p[..., 0][..., None] + torch.cumsum(dp + ddp, dim=-1)\n",
    "        return p_unwrapped\n",
    "    def diff(x):\n",
    "        return x[..., 1:] - x[..., :-1]\n",
    "    n_x = x.size(-1)\n",
    "    hilb_sig = hilbert_torch(x)\n",
    "    \n",
    "    if compute_val == 'power':\n",
    "        out = torch.abs(hilb_sig)\n",
    "        if do_log:\n",
    "            out = torch.log1p(out)\n",
    "    elif compute_val == 'phase':\n",
    "        out = unwrap(angle_custom(hilb_sig))\n",
    "    elif compute_val == 'freqslide':\n",
    "        ang = angle_custom(hilb_sig)\n",
    "        ang = data_srate * diff(unwrap(ang)) / (2 * np.pi)\n",
    "        out = torch.nn.functional.pad(ang, (0, 1), mode='constant')\n",
    "        # TO DO: apply median filter (use torch.median or a custom implementation)\n",
    "    return out\n",
    "\n",
    "# get gpu device\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35014c4b-102b-4051-8aca-5b5fc7ea7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtxNet(nn.Module):\n",
    "    def __init__(self, Chans, Samples=375, dropoutRate=0.65, kernLength=64, F1=4, \n",
    "                 D=2, F2=8, norm_rate=0.25, kernLength_sep=16,\n",
    "                 do_log=False, data_srate=500, base_split=4):\n",
    "        super(CtxNet, self).__init__()\n",
    "        self.do_log = do_log\n",
    "        self.data_srate = data_srate\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.Conv2d(F1, F1*D, (Chans, 1), groups=F1, bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F1*D),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(F1*D, F2, (1, kernLength_sep), bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(self.calculate_flatten_size(Chans, Samples, F2), 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def calculate_flatten_size(self, Chans, Samples, F2):\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, 1, Chans, Samples)\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.block1(x)\n",
    "        x = self.apply_hilbert(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def apply_hilbert(self, x):\n",
    "        return apply_hilbert_torch(x, do_log=self.do_log, compute_val='power', data_srate=self.data_srate)\n",
    "\n",
    "    def calculate_flatten_size(self, Chans, Samples, F2):\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, 1, Chans, Samples)\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.block1(x)\n",
    "        x = self.apply_hilbert(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def apply_hilbert(self, x):\n",
    "        return apply_hilbert_torch(x, do_log=self.do_log, compute_val='power', data_srate=self.data_srate)\n",
    "\n",
    "def create_model(Chans, Samples=375, dropoutRate=0.65, kernLength=64, F1=4, D=2, F2=8):\n",
    "    model = CtxNet(Chans=Chans, Samples=Samples, dropoutRate=dropoutRate, \n",
    "                  kernLength=kernLength, F1=F1, D=D, F2=F2)\n",
    "    return model\n",
    "\n",
    "\n",
    "def positive_correlation_loss(y_true, y_pred, epsilon=1e-8):\n",
    "    mx = torch.mean(y_pred)\n",
    "    my = torch.mean(y_true)\n",
    "    xm, ym = y_pred - mx, y_true - my\n",
    "    # Check if we have enough elements to calculate std\n",
    "    if xm.numel() > 1 and ym.numel() > 1:\n",
    "        r_num = torch.mean(xm * ym)\n",
    "        r_den = torch.std(xm) * torch.std(ym)\n",
    "        r = r_num / (r_den + epsilon)\n",
    "    else:\n",
    "        # If we don't have enough elements, return a default loss\n",
    "        return torch.tensor(1.0, device=y_true.device)\n",
    "    \n",
    "    loss = torch.where(r >= 0, 1 - r, 2 - r)\n",
    "    return loss\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience=7):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def objective(params, X, y, n_splits=4):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "    scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        train_dataset = CustomDataset(X_train, y_train)\n",
    "        val_dataset = CustomDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        model_params = {\n",
    "            'Chans': X.shape[1],\n",
    "            'dropoutRate': params['dropoutRate'],\n",
    "            'kernLength': params['kernLength'],\n",
    "            'F1': params['F1'],\n",
    "            'D': params['D'],\n",
    "            'F2': params['F2']\n",
    "        }\n",
    "        model = create_model(**model_params)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], \n",
    "                                     weight_decay=params['weight_decay'])\n",
    "        criterion = positive_correlation_loss\n",
    "        \n",
    "        trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                    num_epochs=params['max_epochs'], patience=params['patience'])\n",
    "\n",
    "        trained_model.eval()\n",
    "        device = next(trained_model.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = []\n",
    "            for batch in val_loader:\n",
    "                inputs, _ = batch\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = trained_model(inputs)\n",
    "                y_val_pred.extend(outputs.cpu().numpy())\n",
    "            y_val_pred = np.array(y_val_pred).squeeze()\n",
    "        \n",
    "        score = positive_correlation_loss(torch.from_numpy(y_val), torch.from_numpy(y_val_pred)).numpy().item()\n",
    "        scores.append(score)\n",
    "\n",
    "        # Clear memory\n",
    "        del model, trained_model, optimizer, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893d6167-81d1-4bfd-a768-0f2005feaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [\n",
    "    Integer(32, 128, name='kernLength'),\n",
    "    Integer(2, 8, name='F1'),\n",
    "    Integer(1, 8, name='D'),\n",
    "    Integer(4, 16, name='F2'),\n",
    "    Real(0.3, 0.8, name='dropoutRate'),\n",
    "    Categorical([1e-3], name='learning_rate'),  # Fixed at 1e-3\n",
    "    Real(1e-6, 1e-3, \"log-uniform\", name='weight_decay'),\n",
    "    Integer(3, 7, name='patience'),\n",
    "    Categorical([100], name='max_epochs')  # Fixed at 100\n",
    "]\n",
    "f_folder = r'C:\\Users\\admin\\1代码 通过ecog数据预测stn的beta\\data_upload\\demo dataset'\n",
    "batch_size = 16384\n",
    "name = 'ctxnet_demo'\n",
    "y_name = name+'_stnbeta.npy'\n",
    "y_all = np.load(f'{f_folder}\\{y_name}')\n",
    "x_name = name+'_ecog.npy'\n",
    "x_all = np.load(f'{f_folder}\\{x_name}')\n",
    "x_all = x_all.reshape((x_all.shape[0], x_all.shape[1], x_all.shape[2]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d14abec1-9e82-4620-b5b0-1a851e2ea580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14721, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf072732-6ddc-4d0e-86e6-ac3f3a322b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14721, 4, 375)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b597c19-0834-47d8-91a6-b95037f9efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowbeta = y_all[:,0].squeeze()\n",
    "highbeta = y_all[:,1].squeeze()  \n",
    "data = x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33addb96-f4c5-46b3-9cf7-d3933f8efecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "pearsonr_value = []\n",
    "target_list = []\n",
    "sub_list = []\n",
    "file_list = []\n",
    "run_list = []\n",
    "\n",
    "for beta in ['lowbeta', 'highbeta']:\n",
    "    targets = locals()[beta]\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(data, targets, shuffle=False,\n",
    "                                                                test_size=0.3, random_state=42)\n",
    "\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    X_train_val_reshaped = X_train_val.reshape(-1, X_train_val.shape[-1])\n",
    "    X_train_val_normalized = scaler_X.fit_transform(X_train_val_reshaped)\n",
    "    X_train_val_normalized = X_train_val_normalized.reshape(X_train_val.shape)\n",
    "\n",
    "    y_train_val_normalized = scaler_y.fit_transform(y_train_val.reshape(-1, 1)).flatten()\n",
    "\n",
    "    res_gp = gp_minimize(lambda params: objective(dict(zip([dim.name for dim in space], params)), \n",
    "                                                  X_train_val_normalized, y_train_val_normalized),\n",
    "                         space, n_calls=10, random_state=42)\n",
    "\n",
    "    best_params = dict(zip([dim.name for dim in space], res_gp.x))\n",
    "\n",
    "    # Train the final model with the best parameters\n",
    "    model_params = {\n",
    "        'Chans': data.shape[1],\n",
    "        'dropoutRate': best_params['dropoutRate'],\n",
    "        'kernLength': best_params['kernLength'],\n",
    "        'F1': best_params['F1'],\n",
    "        'D': best_params['D'],\n",
    "        'F2': best_params['F2']\n",
    "    }\n",
    "    best_model = create_model(**model_params)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['learning_rate'], \n",
    "                                 weight_decay=best_params['weight_decay'])\n",
    "    criterion = positive_correlation_loss\n",
    "    \n",
    "    train_dataset = CustomDataset(X_train_val_normalized, y_train_val_normalized)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    best_model = train_model(best_model, train_loader, train_loader, criterion, optimizer, \n",
    "                             num_epochs=best_params['max_epochs'], patience=best_params['patience'])\n",
    "    \n",
    "    best_model.eval()\n",
    "    device = next(best_model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        # Predict for y_test\n",
    "        X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "        X_test_normalized = scaler_X.transform(X_test_reshaped)\n",
    "        X_test_normalized = X_test_normalized.reshape(X_test.shape)\n",
    "        y_pred_test_normalized = []\n",
    "        for batch in DataLoader(CustomDataset(X_test_normalized, np.zeros(len(X_test_normalized))), \n",
    "                                batch_size=batch_size):\n",
    "            inputs, _ = batch\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = best_model(inputs)\n",
    "            y_pred_test_normalized.extend(outputs.cpu().numpy())\n",
    "        y_pred_test_normalized = np.array(y_pred_test_normalized)\n",
    "        y_pred_test = scaler_y.inverse_transform(y_pred_test_normalized.reshape(-1, 1)).flatten()\n",
    "\n",
    "        base_filename = os.path.splitext(y_name)[0]\n",
    "        y_test_combined = np.column_stack((y_test, y_pred_test))\n",
    "\n",
    "        # Predict for full dataset (y_all)\n",
    "        X_all_reshaped = x_all.reshape(-1, x_all.shape[-1])\n",
    "        X_all_normalized = scaler_X.transform(X_all_reshaped)\n",
    "        X_all_normalized = X_all_normalized.reshape(x_all.shape)\n",
    "        y_pred_all_normalized = []\n",
    "        for batch in DataLoader(CustomDataset(X_all_normalized, np.zeros(len(X_all_normalized))), \n",
    "                                batch_size=batch_size):\n",
    "            inputs, _ = batch\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = best_model(inputs)\n",
    "            y_pred_all_normalized.extend(outputs.cpu().numpy())\n",
    "        y_pred_all_normalized = np.array(y_pred_all_normalized)\n",
    "        y_pred_all = scaler_y.inverse_transform(y_pred_all_normalized.reshape(-1, 1)).flatten()\n",
    "        y_all_combined = np.column_stack((targets, y_pred_all))\n",
    "\n",
    "    pearsonr_value.append(pearsonr(y_pred_test, y_test)[0])\n",
    "    target_list.append(beta)\n",
    "    run_list.append('test')\n",
    "    sub_list.append('sub-demo')\n",
    "    file_list.append(name)\n",
    "\n",
    "    # Clear memory\n",
    "    del best_model, optimizer, train_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'value': pearsonr_value, \n",
    "    'predict': target_list, \n",
    "    'run': run_list, \n",
    "    'sub': sub_list,\n",
    "    'f_name': file_list\n",
    "})\n",
    "torch.cuda.empty_cache()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb27e34-bd3e-46a4-b881-55adf565002b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24cce93-5ea2-4435-a034-9a9c04a2aff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b08876-b87d-43b1-9319-c7778c0e1f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37a414-9005-4bd9-8fa5-fc643f566328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dde151-6c49-4725-8715-cd1536f1f9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4105039-4bf7-45af-93c9-04cdadf30f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ad138-64bf-4dc0-b15a-3c9dffb2bd77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
