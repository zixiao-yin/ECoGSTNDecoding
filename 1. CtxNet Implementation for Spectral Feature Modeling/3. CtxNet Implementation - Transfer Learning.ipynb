{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ce525-7af1-427c-93d8-03afbd4d6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import pearsonr, zscore\n",
    "import torch.fft as fft\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import gc\n",
    "import ast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94297c05-52bb-4d5a-92f8-847b1d986735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hilbert_torch(x, envelope=False, do_log=False, compute_val='power', data_srate=250):\n",
    "    def hilbert_torch(x):\n",
    "        N = x.size(-1)\n",
    "        \n",
    "        # Disable mixed precision for FFT\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            Xf = fft.fft(x.float(), dim=-1)\n",
    "        \n",
    "        h = torch.zeros(N, dtype=torch.complex64, device=x.device)\n",
    "        if N % 2 == 0:\n",
    "            h[0] = h[N // 2] = 1\n",
    "            h[1:N // 2] = 2\n",
    "        else:\n",
    "            h[0] = 1\n",
    "            h[1:(N + 1) // 2] = 2\n",
    "        \n",
    "        Xf_hilbert = Xf * h\n",
    "        x_hilbert = fft.ifft(Xf_hilbert, dim=-1)\n",
    "        \n",
    "        return x_hilbert\n",
    "    def angle_custom(z):\n",
    "        return torch.atan2(z.imag, z.real)\n",
    "    def unwrap(p, discont=np.pi):\n",
    "        dp = p[..., 1:] - p[..., :-1]\n",
    "        ddp = torch.remainder(dp + np.pi, 2 * np.pi) - np.pi\n",
    "        ddp[torch.abs(dp) < discont] = 0\n",
    "        p_unwrapped = p.clone()\n",
    "        p_unwrapped[..., 1:] = p[..., 0][..., None] + torch.cumsum(dp + ddp, dim=-1)\n",
    "        return p_unwrapped\n",
    "    def diff(x):\n",
    "        return x[..., 1:] - x[..., :-1]\n",
    "    n_x = x.size(-1)\n",
    "    hilb_sig = hilbert_torch(x)\n",
    "    \n",
    "    if compute_val == 'power':\n",
    "        out = torch.abs(hilb_sig)\n",
    "        if do_log:\n",
    "            out = torch.log1p(out)\n",
    "    elif compute_val == 'phase':\n",
    "        out = unwrap(angle_custom(hilb_sig))\n",
    "    elif compute_val == 'freqslide':\n",
    "        ang = angle_custom(hilb_sig)\n",
    "        ang = data_srate * diff(unwrap(ang)) / (2 * np.pi)\n",
    "        out = torch.nn.functional.pad(ang, (0, 1), mode='constant')\n",
    "        # TO DO: apply median filter (use torch.median or a custom implementation)\n",
    "    return out\n",
    "    \n",
    "class CtxNet(nn.Module):\n",
    "    def __init__(self, Chans=3, Samples=375, dropoutRate=0.65, kernLength=64, F1=4, \n",
    "                 D=2, F2=8, F3=16, norm_rate=0.25, kernLength_sep=16,\n",
    "                 do_log=False, data_srate=1, base_split=4):\n",
    "        super(CtxNet, self).__init__()\n",
    "        self.do_log = do_log\n",
    "        self.data_srate = data_srate\n",
    "        \n",
    "        # Block 1 remains the same\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.Conv2d(F1, F1*D, (Chans, 1), groups=F1, bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F1*D),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "        \n",
    "        # Block 2 remains the same\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(F1*D, F2, (1, kernLength_sep), bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "        \n",
    "        # New Block 3\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(F2, F3, (1, kernLength_sep//2), bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Modified to account for block3\n",
    "        flatten_size = self.calculate_flatten_size(Chans, Samples, F3)\n",
    "        \n",
    "        # Enhanced dense layers\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(flatten_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropoutRate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropoutRate)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "                        nn.BatchNorm1d(64),\n",
    "                        nn.Linear(64, 1)\n",
    "                                    )\n",
    "\n",
    "    def calculate_flatten_size(self, Chans, Samples, F3):\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, 1, Chans, Samples)\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "            return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.block1(x)\n",
    "        x = self.apply_hilbert(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)  # Added block3 to forward pass\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def apply_hilbert(self, x):\n",
    "        return apply_hilbert_torch(x, do_log=self.do_log, compute_val='power', data_srate=self.data_srate)\n",
    "\n",
    "def create_model(Chans, Samples=375, dropoutRate=0.65, kernLength=64, F1=4, D=2, F2=8):\n",
    "    model = CtxNet(Chans=Chans, Samples=Samples, dropoutRate=dropoutRate, \n",
    "                 kernLength=kernLength, F1=F1, D=D, F2=F2)\n",
    "    return model\n",
    "\n",
    "def correlation_loss(y_true, y_pred, epsilon=1e-8):\n",
    "    vx = y_pred - y_pred.mean()\n",
    "    vy = y_true - y_true.mean()\n",
    "    corr = (vx * vy).sum() / ((torch.sqrt((vx ** 2).sum()) * torch.sqrt((vy ** 2).sum())) + epsilon)\n",
    "    return 1 - corr\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, normalize=False, scaler=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        if normalize:\n",
    "            # Normalize X across the time dimension\n",
    "            self.X = (self.X - self.X.mean(dim=-1, keepdim=True)) / (self.X.std(dim=-1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            if scaler is None:\n",
    "                # Create new scaler\n",
    "                self.scaler = {\n",
    "                    'y_mean': float(self.y.mean()),\n",
    "                    'y_std': float(self.y.std() + 1e-8)\n",
    "                }\n",
    "            else:\n",
    "                # Use provided scaler\n",
    "                self.scaler = scaler\n",
    "            \n",
    "            # Normalize y using simple standardization\n",
    "            self.y = (self.y - self.scaler['y_mean']) / self.scaler['y_std']\n",
    "        else:\n",
    "            self.scaler = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def inverse_transform_y(self, y):\n",
    "        \"\"\"Transform normalized y back to original scale\"\"\"\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = torch.from_numpy(y).float()\n",
    "        if self.scaler is not None:\n",
    "            return y * self.scaler['y_std'] + self.scaler['y_mean']\n",
    "        return y\n",
    "\n",
    "def train_base_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   model.to(device)\n",
    "   \n",
    "   best_model = None\n",
    "   best_loss = float('inf')\n",
    "   best_correlation = -1\n",
    "   scaler = GradScaler()\n",
    "   \n",
    "   # Learning rate scheduling\n",
    "   initial_lr = optimizer.param_groups[0]['lr']\n",
    "   warmup_epochs = 5\n",
    "   scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, verbose=True)\n",
    "   \n",
    "   # Early stopping\n",
    "   patience = 15\n",
    "   epochs_without_improvement = 0\n",
    "   \n",
    "   for epoch in range(num_epochs):\n",
    "       # Warmup learning rate\n",
    "       if epoch < warmup_epochs:\n",
    "           lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "           for param_group in optimizer.param_groups:\n",
    "               param_group['lr'] = lr\n",
    "               \n",
    "       # Training phase\n",
    "       model.train()\n",
    "       train_loss = 0.0\n",
    "       train_outputs = []\n",
    "       train_targets = []\n",
    "       \n",
    "       for inputs, targets in train_loader:\n",
    "           inputs, targets = inputs.to(device), targets.to(device)\n",
    "           optimizer.zero_grad()\n",
    "           \n",
    "           with autocast():\n",
    "               outputs = model(inputs)\n",
    "               loss = criterion(outputs.squeeze(), targets)\n",
    "           \n",
    "           scaler.scale(loss).backward()\n",
    "           \n",
    "           # Gradient clipping\n",
    "           scaler.unscale_(optimizer)\n",
    "           torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "           \n",
    "           scaler.step(optimizer)\n",
    "           scaler.update()\n",
    "           \n",
    "           train_loss += loss.item()\n",
    "           train_outputs.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "           train_targets.extend(targets.cpu().numpy())\n",
    "           \n",
    "       train_loss /= len(train_loader)\n",
    "       train_correlation = np.corrcoef(train_outputs, train_targets)[0,1]\n",
    "       \n",
    "       # Validation phase\n",
    "       model.eval()\n",
    "       val_loss = 0.0\n",
    "       val_outputs = []\n",
    "       val_targets = []\n",
    "       \n",
    "       with torch.no_grad():\n",
    "           for inputs, targets in val_loader:\n",
    "               inputs, targets = inputs.to(device), targets.to(device)\n",
    "               outputs = model(inputs)\n",
    "               loss = criterion(outputs.squeeze(), targets)\n",
    "               val_loss += loss.item()\n",
    "               val_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "               val_targets.extend(targets.cpu().numpy())\n",
    "       \n",
    "       val_loss /= len(val_loader)\n",
    "       val_correlation = np.corrcoef(val_outputs, val_targets)[0,1]\n",
    "       \n",
    "       # Learning rate scheduling\n",
    "       scheduler.step(val_loss)\n",
    "       \n",
    "       # Model checkpointing\n",
    "       if val_loss < best_loss:\n",
    "           best_loss = val_loss\n",
    "           best_correlation = val_correlation\n",
    "           best_model = copy.deepcopy(model)\n",
    "           epochs_without_improvement = 0\n",
    "           \n",
    "           torch.save({\n",
    "               'epoch': epoch,\n",
    "               'model_state_dict': model.state_dict(),\n",
    "               'optimizer_state_dict': optimizer.state_dict(),\n",
    "               'loss': best_loss,\n",
    "               'correlation': best_correlation,\n",
    "               'scaler_state_dict': scaler.state_dict()\n",
    "           }, 'best_model.pth')\n",
    "       else:\n",
    "           epochs_without_improvement += 1\n",
    "           if epochs_without_improvement >= patience:\n",
    "               print(f'Early stopping at epoch {epoch+1}')\n",
    "               break\n",
    "       \n",
    "       print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "       print(f'Train Loss: {train_loss:.4f}, Train Correlation: {train_correlation:.4f}')\n",
    "       print(f'Val Loss: {val_loss:.4f}, Val Correlation: {val_correlation:.4f}')\n",
    "       print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}\\n')\n",
    "       \n",
    "       # Memory cleanup\n",
    "       torch.cuda.empty_cache()\n",
    "       gc.collect()\n",
    "   \n",
    "   return best_model, best_loss, best_correlation\n",
    "\n",
    "class DataNormalizer:\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def normalize(self, data):\n",
    "        # Z-score normalization with epsilon to prevent division by zero\n",
    "        mean = np.mean(data, axis=(0, 2), keepdims=True)\n",
    "        std = np.std(data, axis=(0, 2), keepdims=True) + self.epsilon\n",
    "        return (data - mean) / std\n",
    "\n",
    "def freeze_layers(model, num_layers_to_freeze):\n",
    "    \"\"\"Freeze initial layers of the model\"\"\"\n",
    "    for i, (name, param) in enumerate(model.named_parameters()):\n",
    "        if i < num_layers_to_freeze:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "    return model\n",
    "\n",
    "class GradualUnfreeze:\n",
    "    def __init__(self, model, total_epochs, unfreeze_layers_per_epoch):\n",
    "        self.model = model\n",
    "        self.total_epochs = total_epochs\n",
    "        self.unfreeze_layers_per_epoch = unfreeze_layers_per_epoch\n",
    "        self.frozen_params = [param for param in model.parameters() if not param.requires_grad]\n",
    "        \n",
    "    def step(self, epoch):\n",
    "        if not self.frozen_params:\n",
    "            return\n",
    "        \n",
    "        layers_to_unfreeze = int(epoch * self.unfreeze_layers_per_epoch)\n",
    "        for i, param in enumerate(self.frozen_params):\n",
    "            if i < layers_to_unfreeze:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "def finetune_model(base_model, train_loader, val_loader, criterion, num_epochs=75):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = copy.deepcopy(base_model)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Freeze initial layers\n",
    "    model = freeze_layers(model, num_layers_to_freeze=6)  # Freeze first two blocks\n",
    "    gradual_unfreeze = GradualUnfreeze(model, num_epochs, unfreeze_layers_per_epoch=2)\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=1e-6,\n",
    "        weight_decay=0.05\n",
    "    )\n",
    "    \n",
    "    # Cosine learning rate scheduler with warm restarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=20,  # Restart every 20 epochs\n",
    "        T_mult=2  # Double the restart interval after each restart\n",
    "    )\n",
    "    step_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    # Gradient clipping\n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 25\n",
    "    min_delta = 0.001  # Minimum improvement required\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        gradual_unfreeze.step(epoch)\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixup augmentation\n",
    "            if epoch > 20:  # Start mixup after 10 epochs\n",
    "                lam = np.random.beta(0.4, 0.4)\n",
    "                idx = torch.randperm(inputs.size(0))\n",
    "                mixed_inputs = lam * inputs + (1 - lam) * inputs[idx]\n",
    "                outputs = model(mixed_inputs)\n",
    "                loss = lam * criterion(outputs.squeeze(), targets) + \\\n",
    "                       (1 - lam) * criterion(outputs.squeeze(), targets[idx])\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        step_scheduler.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss - min_delta:  # Only count as improvement if val_loss decreases by min_delta\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "                break\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def evaluate_model(model, test_loader, denormalize=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs).cpu()  # Move outputs back to CPU immediately\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                print(\"\\nFirst batch statistics (before denormalization):\")\n",
    "                print_data_stats(outputs, \"Model outputs\")\n",
    "                print_data_stats(targets, \"Targets\")\n",
    "            \n",
    "            if denormalize and isinstance(test_loader.dataset, CustomDataset):\n",
    "                outputs = test_loader.dataset.inverse_transform_y(outputs)\n",
    "                targets = test_loader.dataset.inverse_transform_y(targets)\n",
    "                \n",
    "                if batch_idx == 0:\n",
    "                    print(\"\\nFirst batch statistics (after denormalization):\")\n",
    "                    print_data_stats(outputs, \"Model outputs\")\n",
    "                    print_data_stats(targets, \"Targets\")\n",
    "            \n",
    "            all_predictions.extend(outputs.numpy())\n",
    "            all_targets.extend(targets.numpy())\n",
    "    \n",
    "    predictions = np.array(all_predictions).squeeze()\n",
    "    targets = np.array(all_targets)\n",
    "    \n",
    "    print(\"\\nFinal statistics:\")\n",
    "    print_data_stats(torch.tensor(predictions), \"Predictions\")\n",
    "    print_data_stats(torch.tensor(targets), \"Targets\")\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(predictions, targets)[0, 1]\n",
    "    print(f\"\\nCorrelation between predictions and targets: {correlation:.4f}\")\n",
    "    \n",
    "    return predictions, targets\n",
    "    \n",
    "# Modified data loading section\n",
    "def prepare_data(X_name, y_name, index):\n",
    "    X_ini = np.load(X_name).astype(np.float32)[:,index,:]\n",
    "    y_ini = np.load(y_name).astype(np.float32)[:,1]\n",
    "    \n",
    "    normalizer = DataNormalizer()\n",
    "    X_normalized = normalizer.normalize(X_ini)\n",
    "    y_normalized = (y_ini - np.mean(y_ini)) / (np.std(y_ini) + 1e-8)\n",
    "    \n",
    "    return X_normalized, y_normalized\n",
    "    \n",
    "def print_data_stats(data, name):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.from_numpy(data)\n",
    "    print(f\"\\n{name} statistics:\")\n",
    "    print(f\"Mean: {data.mean():.2e}\")\n",
    "    print(f\"Std: {data.std():.2e}\")\n",
    "    print(f\"Min: {data.min():.2e}\")\n",
    "    print(f\"Max: {data.max():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c0df3-c48b-4f1d-9956-55599103702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'Chans': 3,\n",
    "    'kernLength': 64,  # Reduced kernel size\n",
    "    'F1': 32,         # Increased initial filters\n",
    "    'F2': 64,\n",
    "    'F3': 128,\n",
    "    'kernLength_sep': 32,\n",
    "    'dropoutRate': 0.5,\n",
    "    'D': 4,           # Reduced depth multiplier\n",
    "}\n",
    "\n",
    "batch_size = 3072\n",
    "finetune_epochs = 75\n",
    "criterion = correlation_loss\n",
    "ex_folder = r'E:\\data_zixiao\\raw_prediction_61_7'\n",
    "\n",
    "for i in tqdm(range (len(df))):\n",
    "    f_folder = df['folder'][i]\n",
    "    f_name = df['file'][i]\n",
    "    X_name = df['folder'][i] + '\\\\' + df['file'][i][:-4] + '_ecog.npy'\n",
    "    y_name = f_folder + '\\\\' + df['file'][i][:-4] + '_tarstn.npy'\n",
    "    index = ast.literal_eval(df['index_list'][i])\n",
    "    \n",
    "    X_new, y_new = prepare_data(X_name, y_name, index)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.7 * len(X_new))\n",
    "    X_finetune, X_test = X_new[:split_idx], X_new[split_idx:]\n",
    "    y_finetune, y_test = y_new[:split_idx], y_new[split_idx:]\n",
    "    \n",
    "    # Create datasets with normalized data\n",
    "    finetune_dataset = CustomDataset(X_finetune, y_finetune)\n",
    "    test_dataset = CustomDataset(X_test, y_test)\n",
    "    \n",
    "    # Smaller batch size for better generalization\n",
    "    finetune_loader = DataLoader(finetune_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Load and finetune model\n",
    "    trained_base_model = CtxNet(**model_params)\n",
    "    checkpoint = torch.load(r'C:\\zixiao_data\\Ctxnet_base_model_ucsf9_100.pth', weights_only=True)\n",
    "    trained_base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # Increase dropout\n",
    "    for m in trained_base_model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.p = 0.8\n",
    "    for param in trained_base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Only unfreeze final layers\n",
    "    for param in trained_base_model.dense.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in trained_base_model.output.parameters():\n",
    "        param.requires_grad = True\n",
    "    finetuned_model = finetune_model(trained_base_model, finetune_loader, test_loader, criterion)\n",
    "    # Evaluate\n",
    "    y_pred, y_true = evaluate_model(finetuned_model, test_loader)\n",
    "    np.save('%s\\%s'%(ex_folder, f_name[:-8]+'_pred.npy'), np.stack([y_pred, y_true]))\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
